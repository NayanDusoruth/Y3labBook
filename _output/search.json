[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "t About this site"
  },
  {
    "objectID": "testFolder/testpage.html",
    "href": "testFolder/testpage.html",
    "title": "testPage blah balh bal",
    "section": "",
    "text": "testPage blah balh bal\n\n\n\ntestImage\n\n\n%{python} %import os %print(\"hello world\") %print(os.getcwd()) %"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "labBook",
    "section": "",
    "text": "This is a digital lab book for Y3 lab module produced using quarto."
  },
  {
    "objectID": "Wk0/0_0_Wk0_preface.html",
    "href": "Wk0/0_0_Wk0_preface.html",
    "title": "Wk0 preface",
    "section": "",
    "text": "Wk0 preface\nThis Wk0 section is intended as a “miscelaneous” section to this lab book which contains:\n\nGeneral testing - figuring out how to use onenote features to write this labbook\nBibliography\nGeneral pre-lab prep\nGeneral lab info\n\nAs such it’s a mix of more administrative information and pre-lab preparation.",
    "crumbs": [
      "Wk0",
      "Wk0 Preface"
    ]
  },
  {
    "objectID": "Wk0/0_1_test_page.html",
    "href": "Wk0/0_1_test_page.html",
    "title": "0.0 Test page",
    "section": "",
    "text": "0.0 Test page\nThis page is intended to test oneNote features (I.E, equations, diagrams, tables), and generally decide on “how to lab book” - This page is considered not a part of this lab book and will be removed before submission.\nFormatting and structure:\n\nEach week will have its own section made up of pages indexed . (I.E, week 1 page 3 == 1.3)\n\nSetup a page in Wk0 for references – tabulate entries\n\nEqs, tables, figs… will be indexed by week . (I.E, third Eq of week 2 == Eq. 2.3)\n\nTable test:\n\n\n\n\n\n\n\n\n\nCol 1\nCol 2\nCol 3\n\\(C&lt;0\\)\n\n\n\n\n11\n12\n13\n\\(Ae^{-i\\sqrt{C}x}+ Be^{i\\sqrt{C}x}\\)\n\n\n21\n22\n23\n\\(De^{-ct}\\)\n\n\nblah\n\n\n\n\n\n\nbleh\n\n\n\n\n\n\nawawa\n\n\n\n\n\n\n\n\n\n\nEquation test:\n\\(\\int f(x)dx=F(x)+c\\)\nDrawing on samsung notes on tablet, selecting image and copy pasting:\n\n\n\ntestImage\n\n\nProduces a single image file that can be moved around as one piece. This may be the more practical way to do figures and diagrams. Have setup a samsung notes file for lab book diagrams",
    "crumbs": [
      "Wk0",
      "Wk0.1 Test page"
    ]
  },
  {
    "objectID": "Wk0/0_2_Bibliography.html",
    "href": "Wk0/0_2_Bibliography.html",
    "title": "0.2 Bibliography",
    "section": "",
    "text": "0.2 Bibliography\nA page to keep track of which references are used. Note table 0.1, 0.2 aren’t formatted citations, instead representing an easy way to keep track of references and the information needed to create formatted citations. Note that I intend to use a a python script in my personal library to convert tabulated reference information to .bib files.\nThe tables generally have the following fields: * CitationKey (internal reference name) * Entry type (article, book, online, misc) * Note (personal notes on the citation) * URL (preferably direct link to where I got it from) * ISBN (if relevant) * DOI (if relevant) * Author * Title * bookTitle * Publisher * Year * Journal\nCurrently all bilbiography information is stored in ‘mainBibliography.csv’ (unsure how to get quarto to display it here)",
    "crumbs": [
      "Wk0",
      "Wk0.2 Bibliography"
    ]
  },
  {
    "objectID": "Wk0/0_3_Module_and_assignment_specifics.html",
    "href": "Wk0/0_3_Module_and_assignment_specifics.html",
    "title": "0.3 - Module and assignment specifics",
    "section": "",
    "text": "0.3 - Module and assignment specifics\nCourse details: Unit director: Dr Andrei Sarua (a.sarua@bristol.ac.uk) Technical support: Tom Kennedy (Tom.Kennedy@bristol.ac.uk)\nLab opening hours: 10:00-13:00 14:00-17:00 First lab date: 22/01/2026 Last lab date: 26/02/2026\nAssessment details: Lab book submission due date: 12:30 16/03/2026 Lab report submission due date: 12:30 16/03/2026 Assessed on: - In lab attendance/performance - lab book - final report Marking criteria: - Knowledge & understanding - Intellectual skills - Research & scientific practices - Professional & life skills\nLab details: Mab group: C (Thursday labs) Lab partner: Scarlett Kitchener (up24667@bristol.ac.uk) Lab experiment: Ising model Supervisor: Dr Francesco Turci (f.turci@bristol.ac.uk)\nGuidance on lab book: Digital lab book using onenote; should be continuous diary over project; make sure to copy any handwritten notes into the lab book. Needs to be submitted as exported PDF.\nGuidance on final report: Final report is scientific letter style paper using provided LaTex template. 4 pages total (inc everything, inc references/appendix). Best “example” of style to write in will be published scientific articles in area of study.\nLab handbook experiment description (): The Ising model represents a simplified magnet in which each atom has a spin, which can only take one of two values, ‘up’ and ‘down’. Historically, it played a fundamental role in the theory of phase transitions in statistical physics, since it goes from a non-magnetic state at high temperature to a magnetic state at low temperatures, below the Curie temperature Tc. But there are many surprises, and the model and its close relatives are still central to modern physics. For example, the 2016 Nobel prize was awarded to Kosterlitz and Thouless for the theory of phase transitions in the related XY model (where the spin is still fixed in size, but can have any direction in the x-y plane). The Ising model is discrete and so is particularly suited for investigation by computers. In this laboratory we shall investigate the thermodynamics of the Ising model using the Metropolis Monte- Carlo method. Using this approach, we shall investigate the Ising model in zero, one and two dimensions, seeing how the statistical state changes as temperature and magnetic field are varied. In particular, we will use the Metropolis method to investigate the ‘critical exponents’ in the two-dimensional model, which represent how the magnetization, energy, entropy, heat capacity, etc., change near to the Curie temperature Tc. Optionally, the role of the lattice geometry, dimensionality, and similar models, such as Potts and XY could also be explored. This laboratory is specific to theoretical physics students, and introduces concepts, which will be fundamental in the later, e.g., 4th year Phase Transitions course.",
    "crumbs": [
      "Wk0",
      "Wk0.3 Module and assignment specifics"
    ]
  },
  {
    "objectID": "Wk0/0_4_General_pre_prep_log.html",
    "href": "Wk0/0_4_General_pre_prep_log.html",
    "title": "0.4 General pre-prep log",
    "section": "",
    "text": "0.4 General pre-prep log\n15/01/2026\nAdded recommended textbook to bibliography .\nThis lab explores the dynamics of the ising model; a simple model magnet\n19/01/2026 Briefly discussed lab w/Dr Turci; he noted the “percolation” chapter in as a good starting point since it introduces many ideas relating to phase transitions that will be useful for this project\nBlackboard resources “ising1.pdf” have provided a general overview of background concepts and information. Useful ideas and concepts summarised in “0.5 background theory and concepts.” These are NOT the full background theory for this lab, but represent a useful starting point.\n21/01/2026 Called lab partner to introduce self; went over basic logistics and plans, are now on same page.\nBb materials “earmark” list:\n\n“ising1.pdf”, “ising2.pdf” - recaps basic stat mech theory, and covers basic setup of ising model\n“ising exercise.ppt” - notes metropolis algorithm, alongside some observables to compute\n“finite-size-effects.pdf” - goes over phase transition details - order parameters and critical exponents -\n\nLooking through more of the BB materials; and trying to think of vague plan:\n\nGo through BB “finite size effects.pdf”\nGo through “percolation” chapter - (in retrospect this may take a while) - note section 1.8 supposedly outlines extraction of critical exponents\nFamiliarize self w/python simulation provided\nRefactor code in a .py file - jupyter notebooks are more annoying to work with in my experience\n\nAdditional benefit: lets me use my personal library which may be useful\n\nRefactor/expand code “architecture” generally - some potential ideas:\n\nMAYBE rework simulation to allow arbitrary dimensionality (I.E, 1D, 2D, 3D, 4D…) - suspect there may be interesting comparisons for different dimensionalities\nRework as object orriented - each config could be it’s own object with observable methods (I.E, computeEnergy, magnetisation…) and simulation step method - a “simulation run” class could then act as a wrapper for many config objects over time\n\nMay also need to handle different simulations “in parallel” - I.E, need to vary sim parameter to generate data points\nMaybe set it up s.t configuration is object w/update method; overwrites itself; is a data destructive idea, however should only need a final “equilibrium” configuration for a given set of sim parameters\n\nThen a “dataset” object would be a list of final configs + some metadata (I.E, lattice size, # sim steps, dimensionality…)\n\n\nExport/import method - given the computational nature of this lab, heavy compute runs may occur - being able to save raw simulation runs data in a standard format may save compute time\nAdditional observable computation - current code computes energy and magnetisation - adding additional observables may be useful for investigation, I.E:\n\nHeat capacity\nSusceptibility\nHelmholtz free energy\n\n\nMaybe setup shared github repo w/lab partner",
    "crumbs": [
      "Wk0",
      "Wk0.4 General pre-prep log"
    ]
  },
  {
    "objectID": "Wk0/0_5_background_theory_and_concepts.html",
    "href": "Wk0/0_5_background_theory_and_concepts.html",
    "title": "0.5 Background theory and concepts",
    "section": "",
    "text": "/NOT FINISHED COPYING OVER FROM ONENOTE: NEED TO FIX EQUATIONS/\n\n\nThese notes are NOT the full background for this lab, but represents a quick primer on the ising model, statistical mechanic and associated concepts considered useful prior to this lab’s start. This is informed by and my old Y2 notes informed by unless stated otherwise.\n\n\n\nThe ising model is a model magnet made up of a lattice of magnetic spin states:\n\n\n\nFig 0.1 - Basic ising model illustration - Each lattice point represents a magnetic spin in an up/down state\n\n\nWith each spin state having a value of \\(σ_i=±1\\). Each spin states is affected by it’s neighbours and some global magnetic field B. As such, the system’s total energy follows:\nWith each spin state having a value of σ_i=±1. Each spin states is affected by it’s neighbours and some global magnetic field B. As such, the system’s total energy follows:\nH=−∑_ij▒〖Jσ_i σ_j 〗−∑_i▒〖Bσ_i 〗 Eq. 0.1 - Hamiltonian of ising model # TODO: fix\nWhere the first summation represents nearest neighbour interactions, and the second summation represents aligment to the global magnetic field B. J represents some coupling strength between neighbours.\nImportantly, the Ising model experiences a phase transition; with all elements well alligned at low temperatures, and disorganised at high temperatures. This results in high magnetisations at low temperatures, with 0 magnetisation past some phase transition temperature:\n\n\n\nFig 0.2 - Graph illustrating magnetisation over temperature\n\n\nThis occurs because a magnetised “well aligned” state minimises energy, whilst a non magnetised “disorded” state maximises entropy. As outlined by Eq. 0.8, systems will try to minimise energy and maximise entropy, with these competing dynamics moderated by the system temperature. As such, the ising model is expected to undergo a temperature dependent phase transition.\n\n\n\nStatistical mechanics represents a physics toolset to study systems of many interacting elements.\nKey quantities: There are some key quantities in statistical mechanics worth defining initially:\nH=system energy\nT=system temperature\nk_b=1.38⋅10^(−23) J/K Boltzman constant \nZ=∑exp⁡(−H/(k_b T)) Eq. 0.2 - Partition function\nβ=1/(k_b T) Eq. 0.3\n\n\nGiven that statistical systems are made up of many elements as in fig. 0.1:\nA system’s “microstate” describes the specific configuration of all elements in the system, and have a probability:\np=1/Z exp⁡(−H/(k_b T)) Eq. 0.4 - Boltzman distribution\n“Macrostates” on the other hand represent particular observables of a system (I.E, temperature, total energy, magnetisation…). A given macrostate will be made up of many microstates. As such, macrostate probability is “weighed” by the number of corresponding microstates.\n\n\n\nEntropy is an important quantity in statistical mechanics, and represents some sense of “disorder” of a system:\nS=−k_b∑p ln(p)=dF/dT Eq. 0.5 - Entropy\nS(T)=k_b ln⁡(Z)+U/T Eq. 0.6 - Alternate entropy expression\ndS/dU=1/T Eq. 0.7 - Entropy temperature relation\nImportantly, statistical systems will attempt to both maximise entropy and minimise energy; thus leading to the idea of helmholtz free energy:\nF(U,T)=U−TS(U)=−k_b Tln(Z) Eq. 0.8 - Helmholtz free energy\nThis “balance” between energy U and entropy S, mediated by temperature T generally illustrates the contradiction between a system maximising entropy and minimising energy. As such, The probability of a microstate Eq. 0.4 can be rewritten in terms of the Helmholtz free energy:\np(U,T)=1/Z exp⁡(−βF(U,T)) Eq .0.9 - probability in terms of helmholtz energy\n\n\n\nGiven these ideas, its useful to summarize some observables:\nu= =1/Z Hexp ∑⁡(−βH)=dln(Z)/dβ Eq. 0.10 - Average energy\nC=T dS/dT Eq. 0.11 - Specific heat capacity\n&lt;σ_i&gt; =1/Z∑σ_i exp⁡(−βH) Eq. 0.12 - Average spin (specific to ising model)\nM=∑&lt;σ_i&gt; =1/Z=σ_i exp ∑⁡(−βH)=dF/dβ Eq. 0.13 - Magnetisation (specific to ising model)\n\n\n\nThe hamiltonian Eq. 0.1 is difficult to compute due to the nearest neighbour terms. A useful approximation is “Mean Field Theory”. This incorporates the effect of neighbours into the total magnetic field, creating a “mean field”:\nH=−∑_ij▒〖Jσ_i σ_j 〗−∑_i▒〖Bσ_i 〗\n= −∑_ij▒〖Jσ_i&lt;σ_j&gt;〗−∑_i▒〖Bσ_i 〗\n= −∑_i▒〖B_eff σ_i 〗 Eq. 0.14 - Mean field ising hamiltonian\nB_eff=B+J∑_neigh\n\n\n\n\nIt is useful to consider an ising model with a single element. The hamiltonian of such a system becomes:\nH=−Bσ Eq. 0.16 - Single spin hamiltonian\nSo its statistical properties become:\nZ=exp⁡(−Bβ)+exp⁡(Bβ) Eq. 0.17 - Single spin partition\n&lt;σ&gt; =tanh⁡〖(Bβ) 〗 Eq. 0.18 - single spin average spin\nApplying a mean field theory by substituting in Eq. 0.15 to Eq. 0.18 yields:\n&lt;σ&gt; =tanh⁡〖((B+zJ&lt;σ_j&gt;)β) 〗 Eq. 0.19\nThis cannot be solved algebraically, but may be solved numerically for a stable system. This eventually yields either a trivial &lt;σ≥0 solution, or some magnetic solution with phase transiton:\nT_c=zJ/k_b Eq.0.20 - Single spin transition temperature\n\n\n\nThis lab studies phase transitions in the ising model. Percolations are a simple setup which illustrate the key features of phase transitions, and was fully explained by the “percolations” chapter of .\nA percolation system comprises a lattice of binary ([1] OR [0]) states as illustrated:\n\n\n\nFig 0.3 - illustration of percolation on 2D lattice with length L=5 - Note black cells represent [1], whilst white cells represent [0]\n\n\nEach cell is assigned a state randomly according to a probability:\nP(1)=p Eq. 0.21 p(0)=(1−p) Eq. 0.22\nThese cells form clusters of adjacent cells. Given an infinitely large lattice (unlike the finite lattice in fig 0.3), clusters may be finite in size, or infinite in size, spreading across the lattice. This is somewhat illustrated in fig 0.3, where the cluster for p=15/25 could plausibly extend infinitely beyond the 5x5 lattice.\nAs such, the phase transition of percolation models is between a phase where all clusters are finite in size, and a phase where clusters are infinite and span across the lattice.\n\n\nIt is useful to consider the trivial case of a 1D lattice:\n\n\n\nFig 0.4 - 1D percolation lattice\n\n\nIt is evident that infinite cluster can only occur if all cells are in a [1] state, which for an infinite lattice lim_(L→∞)⁡L would require a “critical occupation probability” p_c=1.\nHowever it is useful to illustrate certain tools to describe phase transitions in percolation.\nFirstly, it is useful to count the number of clusters of a given size:\nN(s,p, L)=# of clusters with s cells, given a probability p, with lattice size L. Eq. 0.23\nWhich in the 1D case evaluates to:\nN_(d=1) (s,p,L)=L(1−p)^2 p^s Eq. 0.24\nIt is also useful to normalise this “cluster number density” as:\nn_(d=1) (s,p)=(N_(d=1) (s,p,L))/L=L(1−p)^2 p^s Eq.0.25\nWhich generally follows as:\n\n\n\nFig 0.5 - Approximate plot of number density n_(d=1) (s,p) as a function of cluster size - note the droppof at the “characteristic cluster size” s_ϵ\n\n\nNote that any cell’s probability of belonging to a cluster of size s follows:\np_cluster (s)=sn(s,p) Eq. 0.26\nThe characteristic cluster size is a useful metric which generally follows:\ns_ϵ=−1/(ln⁡(p)) Eq. 0.27\nIt is also interesting to note some “weighted average cluster size” weighted by cluster size:\nχ(p)=1/N_occupied ∑_(k=1)(N_clusters)▒s_k2 Eq. 0.28\nWhich in the 1D case yields:\nχ_(d=1) (p)=(1+p)/(1−p) Eq. 0.29\n\n\n\nFig 0.6 - rough plot of average cluster size over probability - note how it diverges as p→∞ representing an approach to infinite clusters for p=1\n\n\nFinally it is also useful to note a “correlation function” g(r_i,r_j) which describes the probability of any two cells belonging to the same cluster which relates to the average size:\n∑_(r_i)▒〖g(r_i,r_j)〗=χ(p) Eq. 0.30\n\n\n\nGiven these tools of a 1D percolation case, it is possible to expand to 2D in order to examine these phase transitions a bit more closely.\nParticularly it is useful to characterise phase transitions in terms of order parameters and critical exponents.\nAs a general rule, the “order parameter” represents the n’th derivative of the system’s helmholtz free energy where a discontinuity appears; in the case of percolation, that is the probability of percolation p_∞.\nSuch order parameters can ~ generally be expressed in terms of critical exponents near the critical point:\np_∞∝|├ ​p−p_c ┤|^β Eq 0.31\nWhere β is the critical exponent that characterises this phase transition. In a 2D percolation lattice, it has been found to a value β=5/36\nThe average cluster size also experiences such a discontinuity so may be expressed in terms of its own exponent:\nχ(p)∝|├ ​p−p_c ┤|^(−γ) Eq 0.32\nWith a 2D percolation lattice having an exponent γ=43/18.\n\n\n\nNote that a lot of effects are size dependent; however generally it is useful to extract these effects for an infinite lattice.\nThe 1D lattice in fig. 0.4 may illustrate this. Consider the boundary vs “bulk” cells. The bulk cells have 2 neighbors whilst the boundary cells only have one. As such, the bulk cells are more likely to be part of a cluster than the boundary ones. This noticeably distorts the effects of the system. However this effect becomes more negligeable as the lattice grows, with the number of bulk cells growing faster than the number of boundary cells.\nSuch finite lattice effects do however need to be accounted for, as they reduce the apparent discontinuity in phase transitions, making measurement of the critical exponents more difficult.",
    "crumbs": [
      "Wk0",
      "Wk0.5 Background theory and concepts"
    ]
  },
  {
    "objectID": "Wk0/0_5_background_theory_and_concepts.html#foreword",
    "href": "Wk0/0_5_background_theory_and_concepts.html#foreword",
    "title": "0.5 Background theory and concepts",
    "section": "",
    "text": "These notes are NOT the full background for this lab, but represents a quick primer on the ising model, statistical mechanic and associated concepts considered useful prior to this lab’s start. This is informed by and my old Y2 notes informed by unless stated otherwise.",
    "crumbs": [
      "Wk0",
      "Wk0.5 Background theory and concepts"
    ]
  },
  {
    "objectID": "Wk0/0_5_background_theory_and_concepts.html#statement-of-ising-model",
    "href": "Wk0/0_5_background_theory_and_concepts.html#statement-of-ising-model",
    "title": "0.5 Background theory and concepts",
    "section": "",
    "text": "The ising model is a model magnet made up of a lattice of magnetic spin states:\n\n\n\nFig 0.1 - Basic ising model illustration - Each lattice point represents a magnetic spin in an up/down state\n\n\nWith each spin state having a value of \\(σ_i=±1\\). Each spin states is affected by it’s neighbours and some global magnetic field B. As such, the system’s total energy follows:\nWith each spin state having a value of σ_i=±1. Each spin states is affected by it’s neighbours and some global magnetic field B. As such, the system’s total energy follows:\nH=−∑_ij▒〖Jσ_i σ_j 〗−∑_i▒〖Bσ_i 〗 Eq. 0.1 - Hamiltonian of ising model # TODO: fix\nWhere the first summation represents nearest neighbour interactions, and the second summation represents aligment to the global magnetic field B. J represents some coupling strength between neighbours.\nImportantly, the Ising model experiences a phase transition; with all elements well alligned at low temperatures, and disorganised at high temperatures. This results in high magnetisations at low temperatures, with 0 magnetisation past some phase transition temperature:\n\n\n\nFig 0.2 - Graph illustrating magnetisation over temperature\n\n\nThis occurs because a magnetised “well aligned” state minimises energy, whilst a non magnetised “disorded” state maximises entropy. As outlined by Eq. 0.8, systems will try to minimise energy and maximise entropy, with these competing dynamics moderated by the system temperature. As such, the ising model is expected to undergo a temperature dependent phase transition.",
    "crumbs": [
      "Wk0",
      "Wk0.5 Background theory and concepts"
    ]
  },
  {
    "objectID": "Wk1_22_01_2026/1_1_labLog.html",
    "href": "Wk1_22_01_2026/1_1_labLog.html",
    "title": "1.1 lab session notes",
    "section": "",
    "text": "Lab induction\nDr Turci walks us through introduction\nDiscuss refactoring code to give us a “complete virtual lab apparatus”:\nA complete system which we can use to generate arbitrary configuration datasets for study\nSetup github repo\nStart refactoring/writing code (hopefully finish this quickly - that way rest of lab can be running simulations w/minimal need for more coding)\n\nReasoning for this:\n\nWhat is needed from code seems fairly clear at this point; need to be able to generate arbitrary configurations simulated to equilibration; need to be able to extract arbitrary observables; need to be able to assemble these into datasets over parameter variation\nCurrently still going through “percolation” chapter - understanding of theory isn’t fully there yet, however understanding of code requirements ~ is - therefore start with what is known, and familiarise self with theory more later\n\n\n\n\nTurci introduced us to project: * Gave us overview\nMet up w/lab partner, are trying to figure out project direction and logistical questions: * Setup shared google drive folder to share materials * Setup planning document * General starting aims: * Find critical exponents * Study cluster behavior * Vary over temperature and dimensionality * Agreed to refactor code in terms of config object * Object oriented config object w/pickle library save/load functionality - given that sims may be computationally intensive, saving results for future reanalysis may save compute time * Decided to use inheritance structure; parent config, and 1D, 2D, 3D, 4D config children - sim steps will look different in different dimensions and may be difficult to generalize - hence inheritance structure\nTalked to Turci: * Need to figure out when state converges\n\nSetup code refactoring thus far:\n\nParent config class - has saving/load methods\nSetup 2D config as test child case\n\n\nHave setup code in object oriented approach: Abstract config class 2D config class Have done rudimentary testing\nTo do list: * FIX STATE LIST\n\nClean up code\nImplement higher and lower dimensions classes\nRead up and figure out critical exponents\nOptimisation and factorisation:\n\nFigure out numba python library; speed up system\nMaybe rework observable lists into pandas dataframe? May be more generalisable/easier to work with\nDouble check observable calculations\nAdd “average observables” - need to be able to average over several iterations\nMaybe save multiple config steps?\n\nPotentially setup “dataset” object? Set of config objects along some varied quantity? Save to folder of\nSetup github quarto lab book; switch over to lab book\n\n##Lab log (24/01/2026):\n\nDecided to switch back to original state system for now; ik it works; may need to refactor it later\nGonna try and implement numba on 2D case; test everything in 2D, then can expand out to higher dimensions\nNvm; gonna try and get good bones in, and save all states\n\nFor current use, two general test cases:\n\nstandardTest2000 = Ising model 100x100 cells run for 2000 steps\nstandardTest500 = Ising model 100x100 cells run for 500 steps\nHave implemented saving all states - it does make the saved files significantly larger (I.E, 100x100 model w/100 steps, saving energy/magnetization observables is 32.5Mb) - quick test standardTest2000 s; took several minutes (didn’t time it, but took a while), saved to 192.5 Mb\n\nCurrently that is acceptable; (given 100x100x2000 = 2x10^7 stored cells)) that suggests each cell uses ~20 bytes of storage space; so extrapolating to a 3D case 100^3⋅2000=40Gb - a 4D case may be prohibitive w/~4Tb storage space for the same size;\n\nCould add discard functionality (I.E, don’t need the initial equilibration steps)\nCould only save the observables over that simulation length\nHowever for now having the default “save all raw data” option is good to have\n\n\nThinking of optimisations:\n\nmcMove method\ncalcEnergy method\nBoth are VERY inefficient; in similar ways\nBoth run through entire configuration; both check adjacent cells; currently implemented through for loops; however some steps may be more efficiently done by either numpy array operations, or numba implementations\nIt may also be interesting to see if adjacency checks could be generalised/factorised to any dimension; if that’s possible, inheritance structure becomes unecessary, as the algorithm could be implemented for all dimensionalities\n\nIt would be very useful if generalising is possible; this would allow detailed study of higher dimensional ising models\n\nConfig state storage - currently an array of integer values; am wondering if can switch to a binary representation - would make storage of saved files more efficient (I.E, 4D 100x100x2000 case could optimistically be reduced to 200Gb)\nData discard system - options to discard all but the N most recent configurations - storage data optimisation\n\n\nIf its possible to generalise the dinemsionality of the model; that opens up more lines of investigation\nUnsure which optimisations are worth it. Plan of action:\n\nTry and get numba working\nFamiliarise self w/numba operations\nTry and refactor/optimise above with numba in mind\n\nGot numba working; doing rudimentary tests to see if it improves performance:\n\nAdding numba decorator to mcMove method\nstandardTest500 yields Dt = 96.8 s\nstandardTest500 w/out numba yields: 92.2 s\nThis suggests no significant speedup\n\nSuggests I’m using it incorrectly\n\nTesting standardTest500 w/ provided code;\n\nw/out numba: 38.8 s\nW/ numba: 4.26 s\n\nMaybe object oriented approach is breaking stuff?\n\nChanged mc move; moved all self checks to runSimulation instead; that way mcMove is more “anonymous” and doesn’t need to call self\n\nstandardTest500: 47.8 s\nSignificant improvement\n\nWondering if difference between provided code and own code is due to “additional” code run by “simulationRun” - have commented out plotting and appending observables\n\nstandardTest500: 7.3 s\n\n\nTakeaways:\n\nSince Numba compiles individual functions to machine code; it is advisable for numba accelerated functions/methods to be “streamlined” w/out needing to refer to non accelerated code\n\nUnsure if numba accelerated functions can refer to other accelerated functions\n\nTest case: adjacency code in mcMove; move into separate accelerated method\n\nCan move it to separate function, but numba gets confused if it’s a class method\nstandardTest500: 6.9 s\n\n\n\nNumba seems to work; moving onto some architecture changes: * Current plan is to have inheritance structure w/specific dimension configs hard coded * 3 main differences between dimensions: Dimension of array First initial state Adjacencies of cells * Cell adjacency seems the most complicated to generalise; however tuple indices may allow for * Numba is DIFFICULT to work with; it has the type stringency of a C language w/out significantly useful error messages\nFINALLY got the numba stuff working; still needs optimisatin; however everything is working in a generalised sense; I think a general N dimensional config object is doable now:\n\nstandardTest: 130.6 s\nThis is significantly worse than before; I thing its because of the generalised adjacency changes? Alternatively my laptop is on low battery with other stuff running on background; so may be that instead?\nTBD another day\n\nIf it’s a genuine slowdown, may need to comb through and see if its possible to optimise\nAdjacency code - may need to do that once per dimension case? May help reduce computation? Idk\n\n\n\n\n\nQuick test; still slow on full battery/restarted laptop; needs optimisation\n\nspliting adjacency function “getNeighbourIndices” into two:\n\nGet adjacency maps (array of addition/subtractions to be made) - can be done on basis of dimension once per config object\nAdjacency maps can then be passed into “getNeighbourIndices” to add maps%size to desired index\nSplitting computation should reduce repeat workload of finding adjacency maps\n\ngetArrayVal; can rewrite for loop in terms of array manipulations; unsure if it’d be faster given compilation, but numpy may have more efficient implementations\n\nMaybe setup some default D=1,2,3,4,5 search cases; may be faster for defaults that are likely going to be used in this lab; then remaining code\ngetArrayVal uses a size^{currentDimension} array; it’s the same for configs of same dimension/size; precomputing this for each config may help (instead of recomputing it for each\nflattenArray step may be movable to “getNeighbours” function instead? Demodularises the code, but would reduce repeat operations - could also pass in dimensionality as argument instead of evaluating it locally (again reducing repeat steps)\n\nMaybe combine several functions - I wonder if calls between numba’d functions reverts to python, requiring recompilation and slowing down results\n\nPlan:\n\nCombine config2D into config; code should work generalised:\n\nCareful w/2D indices in mcMove and calcEnergy; need to generalize this\n\nEnsure config object has information about its dimensionality\nHave config object query its own adjacency maps in the constructor (to avoid repetition)\nHave adjacency maps passed into Mcmove, and into getNeighbourIndices\nCombine functionality of getArrayVal into getNeighbours - leave getArrayVal as is for now (may be useful for some general one off computations) but streamline getNeighbours for efficiency as outlined above\nTest\nIf need be, maybe combine getNeighbourIndices into getNeighbours - if call between functions is slowing numba down, this is an efficiency gain (at the cost of modularity and legibility)\n\nIf need be combine this all into mcMove directly; preffer not to; modular code tends to be more legible, but it may be a worthwhile tradeoff; esp given that this code is the “core” of the simulation, and shouldn’t need to be touched once its working properlyi\n\n\n\n\n\nProblem - editing state Nd array is problematic w/numba\n\nWorkaround - don’t edit Nd array in mcMove; have mcMove return list of positions and associated values to run simulation; run simulation can then handle the editing\n\nHave found workaround; involves array.reshape(tuple) instead of np.reshape(array, array) - does require passing through sizes array as tuple; but that can be saved once by the config\n\n\nHave moved some functionality around\nstandardTest500; 83s - better but still not good\nEdited main loop in Mcmove to loop over cells in config rather than over I, j - generalises loop\nstandardTest500; 8.8s - seems that editing main loop vastly improved performance\nQuick test of 1D; broke - have noted #TODO in code\n\n\n\n\n\n\nSeems that editing main loop didn’t iterate over all elements?\n\nFixed it; idk if there was actually a speedup\nstandardTest500; 96s\n\nHave yet to separate adjacency check out\n\nHave seperated out “getNeighbourIndices” into “getNeighbourIndices” and “getAdjacencies” - “getAdjacencies” finds the relative neighbour indices (I.E, [0, +-1]) for a given dimensionality - this is done once per config object and passed through to “getNeighbourIndices” - should reduce computation\n\nSlight improvement: standardTest500: 92s\n\n\nWondering if “nested function calls” is whats causing issues?\n\nQuickly moving everything into mcMove - is a lot less modular, but may be more efficient, so worth testing standardTest500: 87.4s Slight improvement - I suspect not duplicating the array flattening step is improving things More tweaking: standardTest500: 81.2s\n\nMore tweaking:\nstandardTest500: 62.1s\nMore tweaking (merged editArray into mcMoves)\nstandardTest500: 33.3s\nMore tweaking - changes how e^(−cost β)=〖(e〗(−β))(cost) is computed - precomputing 〖(e〗^(−β)) to try and shave some more efficiency\nstandardTest500: 50s; have reverted change - presumably multiplication and one exp is easier on computer than alternative\nMinor testing; commented out “appendConfig(curConfig)” from “runSimulation” - given 500 steps, that gets called a lot; want to quantify how significant a drain it represents\nstandardTest500: 23.4s □ Does represent a not insigificant improvement □ May be including a “do not save” option to runSimulation - have added - should help w/both run times and file sizes\n\n\nTaken a break from optimisation to debug; have gotten it working in 1D case\nGeneral debugging complete; have quickly tested that 1D, 3D and 4D cases work; they do; have added graphing method to handle 3D case and rearranged graphing code a bit\nCode still needs commenting/general tidying, but it is in a baseline “complete” state\nNote; observing significant decrease in speed for higher dimension; setting up new test case:\nstandardTest3D100 = 20x20x20 over 100 steps\nstandardTest3D100: 562.6s (9.3min)\nThis is a significant increase from a 2D case; I suspect this will be significantly worse for even higher dimensions; as such it may be prudent to try and understand finite scaling effects in lower dimensions, and try and extrapolate that understanding to small higher dimensionality models as necessary\nNote; incorrect implenentation of code (removing np.flip(currentIndex) operation on line 215) creates an incredibly interesting behaviour which includes:\n\nSymetry along the x/y axis 3 distinct cluster types; black, white, crosshatched",
    "crumbs": [
      "Wk1",
      "Wk 1.1 Lab log"
    ]
  },
  {
    "objectID": "Wk1_22_01_2026/1_1_labLog.html#potential-plan-written-21012026",
    "href": "Wk1_22_01_2026/1_1_labLog.html#potential-plan-written-21012026",
    "title": "1.1 lab session notes",
    "section": "",
    "text": "Lab induction\nDr Turci walks us through introduction\nDiscuss refactoring code to give us a “complete virtual lab apparatus”:\nA complete system which we can use to generate arbitrary configuration datasets for study\nSetup github repo\nStart refactoring/writing code (hopefully finish this quickly - that way rest of lab can be running simulations w/minimal need for more coding)\n\nReasoning for this:\n\nWhat is needed from code seems fairly clear at this point; need to be able to generate arbitrary configurations simulated to equilibration; need to be able to extract arbitrary observables; need to be able to assemble these into datasets over parameter variation\nCurrently still going through “percolation” chapter - understanding of theory isn’t fully there yet, however understanding of code requirements ~ is - therefore start with what is known, and familiarise self with theory more later",
    "crumbs": [
      "Wk1",
      "Wk 1.1 Lab log"
    ]
  },
  {
    "objectID": "Wk1_22_01_2026/1_1_labLog.html#lab-log-22012026",
    "href": "Wk1_22_01_2026/1_1_labLog.html#lab-log-22012026",
    "title": "1.1 lab session notes",
    "section": "",
    "text": "Turci introduced us to project: * Gave us overview\nMet up w/lab partner, are trying to figure out project direction and logistical questions: * Setup shared google drive folder to share materials * Setup planning document * General starting aims: * Find critical exponents * Study cluster behavior * Vary over temperature and dimensionality * Agreed to refactor code in terms of config object * Object oriented config object w/pickle library save/load functionality - given that sims may be computationally intensive, saving results for future reanalysis may save compute time * Decided to use inheritance structure; parent config, and 1D, 2D, 3D, 4D config children - sim steps will look different in different dimensions and may be difficult to generalize - hence inheritance structure\nTalked to Turci: * Need to figure out when state converges\n\nSetup code refactoring thus far:\n\nParent config class - has saving/load methods\nSetup 2D config as test child case\n\n\nHave setup code in object oriented approach: Abstract config class 2D config class Have done rudimentary testing\nTo do list: * FIX STATE LIST\n\nClean up code\nImplement higher and lower dimensions classes\nRead up and figure out critical exponents\nOptimisation and factorisation:\n\nFigure out numba python library; speed up system\nMaybe rework observable lists into pandas dataframe? May be more generalisable/easier to work with\nDouble check observable calculations\nAdd “average observables” - need to be able to average over several iterations\nMaybe save multiple config steps?\n\nPotentially setup “dataset” object? Set of config objects along some varied quantity? Save to folder of\nSetup github quarto lab book; switch over to lab book\n\n##Lab log (24/01/2026):\n\nDecided to switch back to original state system for now; ik it works; may need to refactor it later\nGonna try and implement numba on 2D case; test everything in 2D, then can expand out to higher dimensions\nNvm; gonna try and get good bones in, and save all states\n\nFor current use, two general test cases:\n\nstandardTest2000 = Ising model 100x100 cells run for 2000 steps\nstandardTest500 = Ising model 100x100 cells run for 500 steps\nHave implemented saving all states - it does make the saved files significantly larger (I.E, 100x100 model w/100 steps, saving energy/magnetization observables is 32.5Mb) - quick test standardTest2000 s; took several minutes (didn’t time it, but took a while), saved to 192.5 Mb\n\nCurrently that is acceptable; (given 100x100x2000 = 2x10^7 stored cells)) that suggests each cell uses ~20 bytes of storage space; so extrapolating to a 3D case 100^3⋅2000=40Gb - a 4D case may be prohibitive w/~4Tb storage space for the same size;\n\nCould add discard functionality (I.E, don’t need the initial equilibration steps)\nCould only save the observables over that simulation length\nHowever for now having the default “save all raw data” option is good to have\n\n\nThinking of optimisations:\n\nmcMove method\ncalcEnergy method\nBoth are VERY inefficient; in similar ways\nBoth run through entire configuration; both check adjacent cells; currently implemented through for loops; however some steps may be more efficiently done by either numpy array operations, or numba implementations\nIt may also be interesting to see if adjacency checks could be generalised/factorised to any dimension; if that’s possible, inheritance structure becomes unecessary, as the algorithm could be implemented for all dimensionalities\n\nIt would be very useful if generalising is possible; this would allow detailed study of higher dimensional ising models\n\nConfig state storage - currently an array of integer values; am wondering if can switch to a binary representation - would make storage of saved files more efficient (I.E, 4D 100x100x2000 case could optimistically be reduced to 200Gb)\nData discard system - options to discard all but the N most recent configurations - storage data optimisation\n\n\nIf its possible to generalise the dinemsionality of the model; that opens up more lines of investigation\nUnsure which optimisations are worth it. Plan of action:\n\nTry and get numba working\nFamiliarise self w/numba operations\nTry and refactor/optimise above with numba in mind\n\nGot numba working; doing rudimentary tests to see if it improves performance:\n\nAdding numba decorator to mcMove method\nstandardTest500 yields Dt = 96.8 s\nstandardTest500 w/out numba yields: 92.2 s\nThis suggests no significant speedup\n\nSuggests I’m using it incorrectly\n\nTesting standardTest500 w/ provided code;\n\nw/out numba: 38.8 s\nW/ numba: 4.26 s\n\nMaybe object oriented approach is breaking stuff?\n\nChanged mc move; moved all self checks to runSimulation instead; that way mcMove is more “anonymous” and doesn’t need to call self\n\nstandardTest500: 47.8 s\nSignificant improvement\n\nWondering if difference between provided code and own code is due to “additional” code run by “simulationRun” - have commented out plotting and appending observables\n\nstandardTest500: 7.3 s\n\n\nTakeaways:\n\nSince Numba compiles individual functions to machine code; it is advisable for numba accelerated functions/methods to be “streamlined” w/out needing to refer to non accelerated code\n\nUnsure if numba accelerated functions can refer to other accelerated functions\n\nTest case: adjacency code in mcMove; move into separate accelerated method\n\nCan move it to separate function, but numba gets confused if it’s a class method\nstandardTest500: 6.9 s\n\n\n\nNumba seems to work; moving onto some architecture changes: * Current plan is to have inheritance structure w/specific dimension configs hard coded * 3 main differences between dimensions: Dimension of array First initial state Adjacencies of cells * Cell adjacency seems the most complicated to generalise; however tuple indices may allow for * Numba is DIFFICULT to work with; it has the type stringency of a C language w/out significantly useful error messages\nFINALLY got the numba stuff working; still needs optimisatin; however everything is working in a generalised sense; I think a general N dimensional config object is doable now:\n\nstandardTest: 130.6 s\nThis is significantly worse than before; I thing its because of the generalised adjacency changes? Alternatively my laptop is on low battery with other stuff running on background; so may be that instead?\nTBD another day\n\nIf it’s a genuine slowdown, may need to comb through and see if its possible to optimise\nAdjacency code - may need to do that once per dimension case? May help reduce computation? Idk",
    "crumbs": [
      "Wk1",
      "Wk 1.1 Lab log"
    ]
  },
  {
    "objectID": "Wk1_22_01_2026/1_1_labLog.html#section",
    "href": "Wk1_22_01_2026/1_1_labLog.html#section",
    "title": "1.1 lab session notes",
    "section": "",
    "text": "Quick test; still slow on full battery/restarted laptop; needs optimisation\n\nspliting adjacency function “getNeighbourIndices” into two:\n\nGet adjacency maps (array of addition/subtractions to be made) - can be done on basis of dimension once per config object\nAdjacency maps can then be passed into “getNeighbourIndices” to add maps%size to desired index\nSplitting computation should reduce repeat workload of finding adjacency maps\n\ngetArrayVal; can rewrite for loop in terms of array manipulations; unsure if it’d be faster given compilation, but numpy may have more efficient implementations\n\nMaybe setup some default D=1,2,3,4,5 search cases; may be faster for defaults that are likely going to be used in this lab; then remaining code\ngetArrayVal uses a size^{currentDimension} array; it’s the same for configs of same dimension/size; precomputing this for each config may help (instead of recomputing it for each\nflattenArray step may be movable to “getNeighbours” function instead? Demodularises the code, but would reduce repeat operations - could also pass in dimensionality as argument instead of evaluating it locally (again reducing repeat steps)\n\nMaybe combine several functions - I wonder if calls between numba’d functions reverts to python, requiring recompilation and slowing down results\n\nPlan:\n\nCombine config2D into config; code should work generalised:\n\nCareful w/2D indices in mcMove and calcEnergy; need to generalize this\n\nEnsure config object has information about its dimensionality\nHave config object query its own adjacency maps in the constructor (to avoid repetition)\nHave adjacency maps passed into Mcmove, and into getNeighbourIndices\nCombine functionality of getArrayVal into getNeighbours - leave getArrayVal as is for now (may be useful for some general one off computations) but streamline getNeighbours for efficiency as outlined above\nTest\nIf need be, maybe combine getNeighbourIndices into getNeighbours - if call between functions is slowing numba down, this is an efficiency gain (at the cost of modularity and legibility)\n\nIf need be combine this all into mcMove directly; preffer not to; modular code tends to be more legible, but it may be a worthwhile tradeoff; esp given that this code is the “core” of the simulation, and shouldn’t need to be touched once its working properlyi\n\n\n\n\n\nProblem - editing state Nd array is problematic w/numba\n\nWorkaround - don’t edit Nd array in mcMove; have mcMove return list of positions and associated values to run simulation; run simulation can then handle the editing\n\nHave found workaround; involves array.reshape(tuple) instead of np.reshape(array, array) - does require passing through sizes array as tuple; but that can be saved once by the config\n\n\nHave moved some functionality around\nstandardTest500; 83s - better but still not good\nEdited main loop in Mcmove to loop over cells in config rather than over I, j - generalises loop\nstandardTest500; 8.8s - seems that editing main loop vastly improved performance\nQuick test of 1D; broke - have noted #TODO in code",
    "crumbs": [
      "Wk1",
      "Wk 1.1 Lab log"
    ]
  },
  {
    "objectID": "Wk1_22_01_2026/1_1_labLog.html#log-1",
    "href": "Wk1_22_01_2026/1_1_labLog.html#log-1",
    "title": "1.1 lab session notes",
    "section": "",
    "text": "Seems that editing main loop didn’t iterate over all elements?\n\nFixed it; idk if there was actually a speedup\nstandardTest500; 96s\n\nHave yet to separate adjacency check out\n\nHave seperated out “getNeighbourIndices” into “getNeighbourIndices” and “getAdjacencies” - “getAdjacencies” finds the relative neighbour indices (I.E, [0, +-1]) for a given dimensionality - this is done once per config object and passed through to “getNeighbourIndices” - should reduce computation\n\nSlight improvement: standardTest500: 92s\n\n\nWondering if “nested function calls” is whats causing issues?\n\nQuickly moving everything into mcMove - is a lot less modular, but may be more efficient, so worth testing standardTest500: 87.4s Slight improvement - I suspect not duplicating the array flattening step is improving things More tweaking: standardTest500: 81.2s\n\nMore tweaking:\nstandardTest500: 62.1s\nMore tweaking (merged editArray into mcMoves)\nstandardTest500: 33.3s\nMore tweaking - changes how e^(−cost β)=〖(e〗(−β))(cost) is computed - precomputing 〖(e〗^(−β)) to try and shave some more efficiency\nstandardTest500: 50s; have reverted change - presumably multiplication and one exp is easier on computer than alternative\nMinor testing; commented out “appendConfig(curConfig)” from “runSimulation” - given 500 steps, that gets called a lot; want to quantify how significant a drain it represents\nstandardTest500: 23.4s □ Does represent a not insigificant improvement □ May be including a “do not save” option to runSimulation - have added - should help w/both run times and file sizes\n\n\nTaken a break from optimisation to debug; have gotten it working in 1D case\nGeneral debugging complete; have quickly tested that 1D, 3D and 4D cases work; they do; have added graphing method to handle 3D case and rearranged graphing code a bit\nCode still needs commenting/general tidying, but it is in a baseline “complete” state\nNote; observing significant decrease in speed for higher dimension; setting up new test case:\nstandardTest3D100 = 20x20x20 over 100 steps\nstandardTest3D100: 562.6s (9.3min)\nThis is a significant increase from a 2D case; I suspect this will be significantly worse for even higher dimensions; as such it may be prudent to try and understand finite scaling effects in lower dimensions, and try and extrapolate that understanding to small higher dimensionality models as necessary\nNote; incorrect implenentation of code (removing np.flip(currentIndex) operation on line 215) creates an incredibly interesting behaviour which includes:\n\nSymetry along the x/y axis 3 distinct cluster types; black, white, crosshatched",
    "crumbs": [
      "Wk1",
      "Wk 1.1 Lab log"
    ]
  },
  {
    "objectID": "Wk2_29_01_2026/2_1_labLog.html",
    "href": "Wk2_29_01_2026/2_1_labLog.html",
    "title": "2.1 Lab session notes",
    "section": "",
    "text": "Have begun switching from onenote labbook to a quarto document. Still hand copy equations for “Wk0.5 background theory and concept”.",
    "crumbs": [
      "Wk2",
      "Wk 2.1 Lab log"
    ]
  },
  {
    "objectID": "Wk2_29_01_2026/2_1_labLog.html#note",
    "href": "Wk2_29_01_2026/2_1_labLog.html#note",
    "title": "2.1 Lab session notes",
    "section": "",
    "text": "Have begun switching from onenote labbook to a quarto document. Still hand copy equations for “Wk0.5 background theory and concept”.",
    "crumbs": [
      "Wk2",
      "Wk 2.1 Lab log"
    ]
  },
  {
    "objectID": "Wk0/0_5_background_theory_and_concepts.html#basics-of-statistical-mechanics",
    "href": "Wk0/0_5_background_theory_and_concepts.html#basics-of-statistical-mechanics",
    "title": "0.5 Background theory and concepts",
    "section": "",
    "text": "Statistical mechanics represents a physics toolset to study systems of many interacting elements.\nKey quantities: There are some key quantities in statistical mechanics worth defining initially:\nH=system energy\nT=system temperature\nk_b=1.38⋅10^(−23) J/K Boltzman constant \nZ=∑exp⁡(−H/(k_b T)) Eq. 0.2 - Partition function\nβ=1/(k_b T) Eq. 0.3\n\n\nGiven that statistical systems are made up of many elements as in fig. 0.1:\nA system’s “microstate” describes the specific configuration of all elements in the system, and have a probability:\np=1/Z exp⁡(−H/(k_b T)) Eq. 0.4 - Boltzman distribution\n“Macrostates” on the other hand represent particular observables of a system (I.E, temperature, total energy, magnetisation…). A given macrostate will be made up of many microstates. As such, macrostate probability is “weighed” by the number of corresponding microstates.\n\n\n\nEntropy is an important quantity in statistical mechanics, and represents some sense of “disorder” of a system:\nS=−k_b∑p ln(p)=dF/dT Eq. 0.5 - Entropy\nS(T)=k_b ln⁡(Z)+U/T Eq. 0.6 - Alternate entropy expression\ndS/dU=1/T Eq. 0.7 - Entropy temperature relation\nImportantly, statistical systems will attempt to both maximise entropy and minimise energy; thus leading to the idea of helmholtz free energy:\nF(U,T)=U−TS(U)=−k_b Tln(Z) Eq. 0.8 - Helmholtz free energy\nThis “balance” between energy U and entropy S, mediated by temperature T generally illustrates the contradiction between a system maximising entropy and minimising energy. As such, The probability of a microstate Eq. 0.4 can be rewritten in terms of the Helmholtz free energy:\np(U,T)=1/Z exp⁡(−βF(U,T)) Eq .0.9 - probability in terms of helmholtz energy\n\n\n\nGiven these ideas, its useful to summarize some observables:\nu= =1/Z Hexp ∑⁡(−βH)=dln(Z)/dβ Eq. 0.10 - Average energy\nC=T dS/dT Eq. 0.11 - Specific heat capacity\n&lt;σ_i&gt; =1/Z∑σ_i exp⁡(−βH) Eq. 0.12 - Average spin (specific to ising model)\nM=∑&lt;σ_i&gt; =1/Z=σ_i exp ∑⁡(−βH)=dF/dβ Eq. 0.13 - Magnetisation (specific to ising model)\n\n\n\nThe hamiltonian Eq. 0.1 is difficult to compute due to the nearest neighbour terms. A useful approximation is “Mean Field Theory”. This incorporates the effect of neighbours into the total magnetic field, creating a “mean field”:\nH=−∑_ij▒〖Jσ_i σ_j 〗−∑_i▒〖Bσ_i 〗\n= −∑_ij▒〖Jσ_i&lt;σ_j&gt;〗−∑_i▒〖Bσ_i 〗\n= −∑_i▒〖B_eff σ_i 〗 Eq. 0.14 - Mean field ising hamiltonian\nB_eff=B+J∑_neigh",
    "crumbs": [
      "Wk0",
      "Wk0.5 Background theory and concepts"
    ]
  },
  {
    "objectID": "Wk0/0_5_background_theory_and_concepts.html#single-spin-limit",
    "href": "Wk0/0_5_background_theory_and_concepts.html#single-spin-limit",
    "title": "0.5 Background theory and concepts",
    "section": "",
    "text": "It is useful to consider an ising model with a single element. The hamiltonian of such a system becomes:\nH=−Bσ Eq. 0.16 - Single spin hamiltonian\nSo its statistical properties become:\nZ=exp⁡(−Bβ)+exp⁡(Bβ) Eq. 0.17 - Single spin partition\n&lt;σ&gt; =tanh⁡〖(Bβ) 〗 Eq. 0.18 - single spin average spin\nApplying a mean field theory by substituting in Eq. 0.15 to Eq. 0.18 yields:\n&lt;σ&gt; =tanh⁡〖((B+zJ&lt;σ_j&gt;)β) 〗 Eq. 0.19\nThis cannot be solved algebraically, but may be solved numerically for a stable system. This eventually yields either a trivial &lt;σ≥0 solution, or some magnetic solution with phase transiton:\nT_c=zJ/k_b Eq.0.20 - Single spin transition temperature",
    "crumbs": [
      "Wk0",
      "Wk0.5 Background theory and concepts"
    ]
  },
  {
    "objectID": "Wk0/0_5_background_theory_and_concepts.html#phase-transitions-in-percolation-setups",
    "href": "Wk0/0_5_background_theory_and_concepts.html#phase-transitions-in-percolation-setups",
    "title": "0.5 Background theory and concepts",
    "section": "",
    "text": "This lab studies phase transitions in the ising model. Percolations are a simple setup which illustrate the key features of phase transitions, and was fully explained by the “percolations” chapter of .\nA percolation system comprises a lattice of binary ([1] OR [0]) states as illustrated:\n\n\n\nFig 0.3 - illustration of percolation on 2D lattice with length L=5 - Note black cells represent [1], whilst white cells represent [0]\n\n\nEach cell is assigned a state randomly according to a probability:\nP(1)=p Eq. 0.21 p(0)=(1−p) Eq. 0.22\nThese cells form clusters of adjacent cells. Given an infinitely large lattice (unlike the finite lattice in fig 0.3), clusters may be finite in size, or infinite in size, spreading across the lattice. This is somewhat illustrated in fig 0.3, where the cluster for p=15/25 could plausibly extend infinitely beyond the 5x5 lattice.\nAs such, the phase transition of percolation models is between a phase where all clusters are finite in size, and a phase where clusters are infinite and span across the lattice.\n\n\nIt is useful to consider the trivial case of a 1D lattice:\n\n\n\nFig 0.4 - 1D percolation lattice\n\n\nIt is evident that infinite cluster can only occur if all cells are in a [1] state, which for an infinite lattice lim_(L→∞)⁡L would require a “critical occupation probability” p_c=1.\nHowever it is useful to illustrate certain tools to describe phase transitions in percolation.\nFirstly, it is useful to count the number of clusters of a given size:\nN(s,p, L)=# of clusters with s cells, given a probability p, with lattice size L. Eq. 0.23\nWhich in the 1D case evaluates to:\nN_(d=1) (s,p,L)=L(1−p)^2 p^s Eq. 0.24\nIt is also useful to normalise this “cluster number density” as:\nn_(d=1) (s,p)=(N_(d=1) (s,p,L))/L=L(1−p)^2 p^s Eq.0.25\nWhich generally follows as:\n\n\n\nFig 0.5 - Approximate plot of number density n_(d=1) (s,p) as a function of cluster size - note the droppof at the “characteristic cluster size” s_ϵ\n\n\nNote that any cell’s probability of belonging to a cluster of size s follows:\np_cluster (s)=sn(s,p) Eq. 0.26\nThe characteristic cluster size is a useful metric which generally follows:\ns_ϵ=−1/(ln⁡(p)) Eq. 0.27\nIt is also interesting to note some “weighted average cluster size” weighted by cluster size:\nχ(p)=1/N_occupied ∑_(k=1)(N_clusters)▒s_k2 Eq. 0.28\nWhich in the 1D case yields:\nχ_(d=1) (p)=(1+p)/(1−p) Eq. 0.29\n\n\n\nFig 0.6 - rough plot of average cluster size over probability - note how it diverges as p→∞ representing an approach to infinite clusters for p=1\n\n\nFinally it is also useful to note a “correlation function” g(r_i,r_j) which describes the probability of any two cells belonging to the same cluster which relates to the average size:\n∑_(r_i)▒〖g(r_i,r_j)〗=χ(p) Eq. 0.30\n\n\n\nGiven these tools of a 1D percolation case, it is possible to expand to 2D in order to examine these phase transitions a bit more closely.\nParticularly it is useful to characterise phase transitions in terms of order parameters and critical exponents.\nAs a general rule, the “order parameter” represents the n’th derivative of the system’s helmholtz free energy where a discontinuity appears; in the case of percolation, that is the probability of percolation p_∞.\nSuch order parameters can ~ generally be expressed in terms of critical exponents near the critical point:\np_∞∝|├ ​p−p_c ┤|^β Eq 0.31\nWhere β is the critical exponent that characterises this phase transition. In a 2D percolation lattice, it has been found to a value β=5/36\nThe average cluster size also experiences such a discontinuity so may be expressed in terms of its own exponent:\nχ(p)∝|├ ​p−p_c ┤|^(−γ) Eq 0.32\nWith a 2D percolation lattice having an exponent γ=43/18.\n\n\n\nNote that a lot of effects are size dependent; however generally it is useful to extract these effects for an infinite lattice.\nThe 1D lattice in fig. 0.4 may illustrate this. Consider the boundary vs “bulk” cells. The bulk cells have 2 neighbors whilst the boundary cells only have one. As such, the bulk cells are more likely to be part of a cluster than the boundary ones. This noticeably distorts the effects of the system. However this effect becomes more negligeable as the lattice grows, with the number of bulk cells growing faster than the number of boundary cells.\nSuch finite lattice effects do however need to be accounted for, as they reduce the apparent discontinuity in phase transitions, making measurement of the critical exponents more difficult.",
    "crumbs": [
      "Wk0",
      "Wk0.5 Background theory and concepts"
    ]
  }
]