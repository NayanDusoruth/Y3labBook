[
  {
    "objectID": "Wk0/0_1_test_page.html",
    "href": "Wk0/0_1_test_page.html",
    "title": "0.0 Test page",
    "section": "",
    "text": "0.0 Test page\nThis page is intended to test oneNote features (I.E, equations, diagrams, tables), and generally decide on “how to lab book” - This page is considered not a part of this lab book and will be removed before submission.\nFormatting and structure:\n\nEach week will have its own section made up of pages indexed . (I.E, week 1 page 3 == 1.3)\n\nSetup a page in Wk0 for references – tabulate entries\n\nEqs, tables, figs… will be indexed by week . (I.E, third Eq of week 2 == Eq. 2.3)\n\nTable test:\n\n\n\n\n\n\n\n\n\nCol 1\nCol 2\nCol 3\n\\(C&lt;0\\)\n\n\n\n\n11\n12\n13\n\\(Ae^{-i\\sqrt{C}x}+ Be^{i\\sqrt{C}x}\\)\n\n\n21\n22\n23\n\\(De^{-ct}\\)\n\n\nblah\n\n\n\n\n\n\nbleh\n\n\n\n\n\n\nawawa\n\n\n\n\n\n\n\n\n\n\nEquation test:\n\\(\\int f(x)dx=F(x)+c\\)\nDrawing on samsung notes on tablet, selecting image and copy pasting:\n\n\n\ntestImage\n\n\nProduces a single image file that can be moved around as one piece. This may be the more practical way to do figures and diagrams. Have setup a samsung notes file for lab book diagrams",
    "crumbs": [
      "Wk0",
      "Wk0.1 Test page"
    ]
  },
  {
    "objectID": "Wk0/0_5_background_theory_and_concepts.html",
    "href": "Wk0/0_5_background_theory_and_concepts.html",
    "title": "0.5 Background theory and concepts",
    "section": "",
    "text": "/NOT FINISHED COPYING OVER FROM ONENOTE: NEED TO FIX EQUATIONS/\n\n\nThese notes are NOT the full background for this lab, but represents a quick primer on the ising model, statistical mechanic and associated concepts considered useful prior to this lab’s start. This is informed by and my old Y2 notes informed by unless stated otherwise.\n\n\n\nThe ising model is a model magnet made up of a lattice of magnetic spin states:\n\n\n\nFig 0.1 - Basic ising model illustration - Each lattice point represents a magnetic spin in an up/down state\n\n\nWith each spin state having a value of \\(σ_i=±1\\). Each spin states is affected by it’s neighbours and some global magnetic field B. As such, the system’s total energy follows:\nWith each spin state having a value of σ_i=±1. Each spin states is affected by it’s neighbours and some global magnetic field B. As such, the system’s total energy follows:\nH=−∑_ij▒〖Jσ_i σ_j 〗−∑_i▒〖Bσ_i 〗 Eq. 0.1 - Hamiltonian of ising model # TODO: fix\nWhere the first summation represents nearest neighbour interactions, and the second summation represents aligment to the global magnetic field B. J represents some coupling strength between neighbours.\nImportantly, the Ising model experiences a phase transition; with all elements well alligned at low temperatures, and disorganised at high temperatures. This results in high magnetisations at low temperatures, with 0 magnetisation past some phase transition temperature:\n\n\n\nFig 0.2 - Graph illustrating magnetisation over temperature\n\n\nThis occurs because a magnetised “well aligned” state minimises energy, whilst a non magnetised “disorded” state maximises entropy. As outlined by Eq. 0.8, systems will try to minimise energy and maximise entropy, with these competing dynamics moderated by the system temperature. As such, the ising model is expected to undergo a temperature dependent phase transition.\n\n\n\nStatistical mechanics represents a physics toolset to study systems of many interacting elements.\nKey quantities: There are some key quantities in statistical mechanics worth defining initially:\nH=system energy\nT=system temperature\nk_b=1.38⋅10^(−23) J/K Boltzman constant \nZ=∑exp⁡(−H/(k_b T)) Eq. 0.2 - Partition function\nβ=1/(k_b T) Eq. 0.3\n\n\nGiven that statistical systems are made up of many elements as in fig. 0.1:\nA system’s “microstate” describes the specific configuration of all elements in the system, and have a probability:\np=1/Z exp⁡(−H/(k_b T)) Eq. 0.4 - Boltzman distribution\n“Macrostates” on the other hand represent particular observables of a system (I.E, temperature, total energy, magnetisation…). A given macrostate will be made up of many microstates. As such, macrostate probability is “weighed” by the number of corresponding microstates.\n\n\n\nEntropy is an important quantity in statistical mechanics, and represents some sense of “disorder” of a system:\nS=−k_b∑p ln(p)=dF/dT Eq. 0.5 - Entropy\nS(T)=k_b ln⁡(Z)+U/T Eq. 0.6 - Alternate entropy expression\ndS/dU=1/T Eq. 0.7 - Entropy temperature relation\nImportantly, statistical systems will attempt to both maximise entropy and minimise energy; thus leading to the idea of helmholtz free energy:\nF(U,T)=U−TS(U)=−k_b Tln(Z) Eq. 0.8 - Helmholtz free energy\nThis “balance” between energy U and entropy S, mediated by temperature T generally illustrates the contradiction between a system maximising entropy and minimising energy. As such, The probability of a microstate Eq. 0.4 can be rewritten in terms of the Helmholtz free energy:\np(U,T)=1/Z exp⁡(−βF(U,T)) Eq .0.9 - probability in terms of helmholtz energy\n\n\n\nGiven these ideas, its useful to summarize some observables:\nu= =1/Z Hexp ∑⁡(−βH)=dln(Z)/dβ Eq. 0.10 - Average energy\nC=T dS/dT Eq. 0.11 - Specific heat capacity\n&lt;σ_i&gt; =1/Z∑σ_i exp⁡(−βH) Eq. 0.12 - Average spin (specific to ising model)\nM=∑&lt;σ_i&gt; =1/Z=σ_i exp ∑⁡(−βH)=dF/dβ Eq. 0.13 - Magnetisation (specific to ising model)\n\n\n\nThe hamiltonian Eq. 0.1 is difficult to compute due to the nearest neighbour terms. A useful approximation is “Mean Field Theory”. This incorporates the effect of neighbours into the total magnetic field, creating a “mean field”:\nH=−∑_ij▒〖Jσ_i σ_j 〗−∑_i▒〖Bσ_i 〗\n= −∑_ij▒〖Jσ_i&lt;σ_j&gt;〗−∑_i▒〖Bσ_i 〗\n= −∑_i▒〖B_eff σ_i 〗 Eq. 0.14 - Mean field ising hamiltonian\nB_eff=B+J∑_neigh\n\n\n\n\nIt is useful to consider an ising model with a single element. The hamiltonian of such a system becomes:\nH=−Bσ Eq. 0.16 - Single spin hamiltonian\nSo its statistical properties become:\nZ=exp⁡(−Bβ)+exp⁡(Bβ) Eq. 0.17 - Single spin partition\n&lt;σ&gt; =tanh⁡〖(Bβ) 〗 Eq. 0.18 - single spin average spin\nApplying a mean field theory by substituting in Eq. 0.15 to Eq. 0.18 yields:\n&lt;σ&gt; =tanh⁡〖((B+zJ&lt;σ_j&gt;)β) 〗 Eq. 0.19\nThis cannot be solved algebraically, but may be solved numerically for a stable system. This eventually yields either a trivial &lt;σ≥0 solution, or some magnetic solution with phase transiton:\nT_c=zJ/k_b Eq.0.20 - Single spin transition temperature\n\n\n\nThis lab studies phase transitions in the ising model. Percolations are a simple setup which illustrate the key features of phase transitions, and was fully explained by the “percolations” chapter of .\nA percolation system comprises a lattice of binary ([1] OR [0]) states as illustrated:\n\n\n\nFig 0.3 - illustration of percolation on 2D lattice with length L=5 - Note black cells represent [1], whilst white cells represent [0]\n\n\nEach cell is assigned a state randomly according to a probability:\nP(1)=p Eq. 0.21 p(0)=(1−p) Eq. 0.22\nThese cells form clusters of adjacent cells. Given an infinitely large lattice (unlike the finite lattice in fig 0.3), clusters may be finite in size, or infinite in size, spreading across the lattice. This is somewhat illustrated in fig 0.3, where the cluster for p=15/25 could plausibly extend infinitely beyond the 5x5 lattice.\nAs such, the phase transition of percolation models is between a phase where all clusters are finite in size, and a phase where clusters are infinite and span across the lattice.\n\n\nIt is useful to consider the trivial case of a 1D lattice:\n\n\n\nFig 0.4 - 1D percolation lattice\n\n\nIt is evident that infinite cluster can only occur if all cells are in a [1] state, which for an infinite lattice lim_(L→∞)⁡L would require a “critical occupation probability” p_c=1.\nHowever it is useful to illustrate certain tools to describe phase transitions in percolation.\nFirstly, it is useful to count the number of clusters of a given size:\nN(s,p, L)=# of clusters with s cells, given a probability p, with lattice size L. Eq. 0.23\nWhich in the 1D case evaluates to:\nN_(d=1) (s,p,L)=L(1−p)^2 p^s Eq. 0.24\nIt is also useful to normalise this “cluster number density” as:\nn_(d=1) (s,p)=(N_(d=1) (s,p,L))/L=L(1−p)^2 p^s Eq.0.25\nWhich generally follows as:\n\n\n\nFig 0.5 - Approximate plot of number density n_(d=1) (s,p) as a function of cluster size - note the droppof at the “characteristic cluster size” s_ϵ\n\n\nNote that any cell’s probability of belonging to a cluster of size s follows:\np_cluster (s)=sn(s,p) Eq. 0.26\nThe characteristic cluster size is a useful metric which generally follows:\ns_ϵ=−1/(ln⁡(p)) Eq. 0.27\nIt is also interesting to note some “weighted average cluster size” weighted by cluster size:\nχ(p)=1/N_occupied ∑_(k=1)(N_clusters)▒s_k2 Eq. 0.28\nWhich in the 1D case yields:\nχ_(d=1) (p)=(1+p)/(1−p) Eq. 0.29\n\n\n\nFig 0.6 - rough plot of average cluster size over probability - note how it diverges as p→∞ representing an approach to infinite clusters for p=1\n\n\nFinally it is also useful to note a “correlation function” g(r_i,r_j) which describes the probability of any two cells belonging to the same cluster which relates to the average size:\n∑_(r_i)▒〖g(r_i,r_j)〗=χ(p) Eq. 0.30\n\n\n\nGiven these tools of a 1D percolation case, it is possible to expand to 2D in order to examine these phase transitions a bit more closely.\nParticularly it is useful to characterise phase transitions in terms of order parameters and critical exponents.\nAs a general rule, the “order parameter” represents the n’th derivative of the system’s helmholtz free energy where a discontinuity appears; in the case of percolation, that is the probability of percolation p_∞.\nSuch order parameters can ~ generally be expressed in terms of critical exponents near the critical point:\np_∞∝|├ ​p−p_c ┤|^β Eq 0.31\nWhere β is the critical exponent that characterises this phase transition. In a 2D percolation lattice, it has been found to a value β=5/36\nThe average cluster size also experiences such a discontinuity so may be expressed in terms of its own exponent:\nχ(p)∝|├ ​p−p_c ┤|^(−γ) Eq 0.32\nWith a 2D percolation lattice having an exponent γ=43/18.\n\n\n\nNote that a lot of effects are size dependent; however generally it is useful to extract these effects for an infinite lattice.\nThe 1D lattice in fig. 0.4 may illustrate this. Consider the boundary vs “bulk” cells. The bulk cells have 2 neighbors whilst the boundary cells only have one. As such, the bulk cells are more likely to be part of a cluster than the boundary ones. This noticeably distorts the effects of the system. However this effect becomes more negligeable as the lattice grows, with the number of bulk cells growing faster than the number of boundary cells.\nSuch finite lattice effects do however need to be accounted for, as they reduce the apparent discontinuity in phase transitions, making measurement of the critical exponents more difficult.",
    "crumbs": [
      "Wk0",
      "Wk0.5 Background theory and concepts"
    ]
  },
  {
    "objectID": "Wk0/0_5_background_theory_and_concepts.html#foreword",
    "href": "Wk0/0_5_background_theory_and_concepts.html#foreword",
    "title": "0.5 Background theory and concepts",
    "section": "",
    "text": "These notes are NOT the full background for this lab, but represents a quick primer on the ising model, statistical mechanic and associated concepts considered useful prior to this lab’s start. This is informed by and my old Y2 notes informed by unless stated otherwise.",
    "crumbs": [
      "Wk0",
      "Wk0.5 Background theory and concepts"
    ]
  },
  {
    "objectID": "Wk0/0_5_background_theory_and_concepts.html#statement-of-ising-model",
    "href": "Wk0/0_5_background_theory_and_concepts.html#statement-of-ising-model",
    "title": "0.5 Background theory and concepts",
    "section": "",
    "text": "The ising model is a model magnet made up of a lattice of magnetic spin states:\n\n\n\nFig 0.1 - Basic ising model illustration - Each lattice point represents a magnetic spin in an up/down state\n\n\nWith each spin state having a value of \\(σ_i=±1\\). Each spin states is affected by it’s neighbours and some global magnetic field B. As such, the system’s total energy follows:\nWith each spin state having a value of σ_i=±1. Each spin states is affected by it’s neighbours and some global magnetic field B. As such, the system’s total energy follows:\nH=−∑_ij▒〖Jσ_i σ_j 〗−∑_i▒〖Bσ_i 〗 Eq. 0.1 - Hamiltonian of ising model # TODO: fix\nWhere the first summation represents nearest neighbour interactions, and the second summation represents aligment to the global magnetic field B. J represents some coupling strength between neighbours.\nImportantly, the Ising model experiences a phase transition; with all elements well alligned at low temperatures, and disorganised at high temperatures. This results in high magnetisations at low temperatures, with 0 magnetisation past some phase transition temperature:\n\n\n\nFig 0.2 - Graph illustrating magnetisation over temperature\n\n\nThis occurs because a magnetised “well aligned” state minimises energy, whilst a non magnetised “disorded” state maximises entropy. As outlined by Eq. 0.8, systems will try to minimise energy and maximise entropy, with these competing dynamics moderated by the system temperature. As such, the ising model is expected to undergo a temperature dependent phase transition.",
    "crumbs": [
      "Wk0",
      "Wk0.5 Background theory and concepts"
    ]
  },
  {
    "objectID": "Wk0/0_5_background_theory_and_concepts.html#basics-of-statistical-mechanics",
    "href": "Wk0/0_5_background_theory_and_concepts.html#basics-of-statistical-mechanics",
    "title": "0.5 Background theory and concepts",
    "section": "",
    "text": "Statistical mechanics represents a physics toolset to study systems of many interacting elements.\nKey quantities: There are some key quantities in statistical mechanics worth defining initially:\nH=system energy\nT=system temperature\nk_b=1.38⋅10^(−23) J/K Boltzman constant \nZ=∑exp⁡(−H/(k_b T)) Eq. 0.2 - Partition function\nβ=1/(k_b T) Eq. 0.3\n\n\nGiven that statistical systems are made up of many elements as in fig. 0.1:\nA system’s “microstate” describes the specific configuration of all elements in the system, and have a probability:\np=1/Z exp⁡(−H/(k_b T)) Eq. 0.4 - Boltzman distribution\n“Macrostates” on the other hand represent particular observables of a system (I.E, temperature, total energy, magnetisation…). A given macrostate will be made up of many microstates. As such, macrostate probability is “weighed” by the number of corresponding microstates.\n\n\n\nEntropy is an important quantity in statistical mechanics, and represents some sense of “disorder” of a system:\nS=−k_b∑p ln(p)=dF/dT Eq. 0.5 - Entropy\nS(T)=k_b ln⁡(Z)+U/T Eq. 0.6 - Alternate entropy expression\ndS/dU=1/T Eq. 0.7 - Entropy temperature relation\nImportantly, statistical systems will attempt to both maximise entropy and minimise energy; thus leading to the idea of helmholtz free energy:\nF(U,T)=U−TS(U)=−k_b Tln(Z) Eq. 0.8 - Helmholtz free energy\nThis “balance” between energy U and entropy S, mediated by temperature T generally illustrates the contradiction between a system maximising entropy and minimising energy. As such, The probability of a microstate Eq. 0.4 can be rewritten in terms of the Helmholtz free energy:\np(U,T)=1/Z exp⁡(−βF(U,T)) Eq .0.9 - probability in terms of helmholtz energy\n\n\n\nGiven these ideas, its useful to summarize some observables:\nu= =1/Z Hexp ∑⁡(−βH)=dln(Z)/dβ Eq. 0.10 - Average energy\nC=T dS/dT Eq. 0.11 - Specific heat capacity\n&lt;σ_i&gt; =1/Z∑σ_i exp⁡(−βH) Eq. 0.12 - Average spin (specific to ising model)\nM=∑&lt;σ_i&gt; =1/Z=σ_i exp ∑⁡(−βH)=dF/dβ Eq. 0.13 - Magnetisation (specific to ising model)\n\n\n\nThe hamiltonian Eq. 0.1 is difficult to compute due to the nearest neighbour terms. A useful approximation is “Mean Field Theory”. This incorporates the effect of neighbours into the total magnetic field, creating a “mean field”:\nH=−∑_ij▒〖Jσ_i σ_j 〗−∑_i▒〖Bσ_i 〗\n= −∑_ij▒〖Jσ_i&lt;σ_j&gt;〗−∑_i▒〖Bσ_i 〗\n= −∑_i▒〖B_eff σ_i 〗 Eq. 0.14 - Mean field ising hamiltonian\nB_eff=B+J∑_neigh",
    "crumbs": [
      "Wk0",
      "Wk0.5 Background theory and concepts"
    ]
  },
  {
    "objectID": "Wk0/0_5_background_theory_and_concepts.html#single-spin-limit",
    "href": "Wk0/0_5_background_theory_and_concepts.html#single-spin-limit",
    "title": "0.5 Background theory and concepts",
    "section": "",
    "text": "It is useful to consider an ising model with a single element. The hamiltonian of such a system becomes:\nH=−Bσ Eq. 0.16 - Single spin hamiltonian\nSo its statistical properties become:\nZ=exp⁡(−Bβ)+exp⁡(Bβ) Eq. 0.17 - Single spin partition\n&lt;σ&gt; =tanh⁡〖(Bβ) 〗 Eq. 0.18 - single spin average spin\nApplying a mean field theory by substituting in Eq. 0.15 to Eq. 0.18 yields:\n&lt;σ&gt; =tanh⁡〖((B+zJ&lt;σ_j&gt;)β) 〗 Eq. 0.19\nThis cannot be solved algebraically, but may be solved numerically for a stable system. This eventually yields either a trivial &lt;σ≥0 solution, or some magnetic solution with phase transiton:\nT_c=zJ/k_b Eq.0.20 - Single spin transition temperature",
    "crumbs": [
      "Wk0",
      "Wk0.5 Background theory and concepts"
    ]
  },
  {
    "objectID": "Wk0/0_5_background_theory_and_concepts.html#phase-transitions-in-percolation-setups",
    "href": "Wk0/0_5_background_theory_and_concepts.html#phase-transitions-in-percolation-setups",
    "title": "0.5 Background theory and concepts",
    "section": "",
    "text": "This lab studies phase transitions in the ising model. Percolations are a simple setup which illustrate the key features of phase transitions, and was fully explained by the “percolations” chapter of .\nA percolation system comprises a lattice of binary ([1] OR [0]) states as illustrated:\n\n\n\nFig 0.3 - illustration of percolation on 2D lattice with length L=5 - Note black cells represent [1], whilst white cells represent [0]\n\n\nEach cell is assigned a state randomly according to a probability:\nP(1)=p Eq. 0.21 p(0)=(1−p) Eq. 0.22\nThese cells form clusters of adjacent cells. Given an infinitely large lattice (unlike the finite lattice in fig 0.3), clusters may be finite in size, or infinite in size, spreading across the lattice. This is somewhat illustrated in fig 0.3, where the cluster for p=15/25 could plausibly extend infinitely beyond the 5x5 lattice.\nAs such, the phase transition of percolation models is between a phase where all clusters are finite in size, and a phase where clusters are infinite and span across the lattice.\n\n\nIt is useful to consider the trivial case of a 1D lattice:\n\n\n\nFig 0.4 - 1D percolation lattice\n\n\nIt is evident that infinite cluster can only occur if all cells are in a [1] state, which for an infinite lattice lim_(L→∞)⁡L would require a “critical occupation probability” p_c=1.\nHowever it is useful to illustrate certain tools to describe phase transitions in percolation.\nFirstly, it is useful to count the number of clusters of a given size:\nN(s,p, L)=# of clusters with s cells, given a probability p, with lattice size L. Eq. 0.23\nWhich in the 1D case evaluates to:\nN_(d=1) (s,p,L)=L(1−p)^2 p^s Eq. 0.24\nIt is also useful to normalise this “cluster number density” as:\nn_(d=1) (s,p)=(N_(d=1) (s,p,L))/L=L(1−p)^2 p^s Eq.0.25\nWhich generally follows as:\n\n\n\nFig 0.5 - Approximate plot of number density n_(d=1) (s,p) as a function of cluster size - note the droppof at the “characteristic cluster size” s_ϵ\n\n\nNote that any cell’s probability of belonging to a cluster of size s follows:\np_cluster (s)=sn(s,p) Eq. 0.26\nThe characteristic cluster size is a useful metric which generally follows:\ns_ϵ=−1/(ln⁡(p)) Eq. 0.27\nIt is also interesting to note some “weighted average cluster size” weighted by cluster size:\nχ(p)=1/N_occupied ∑_(k=1)(N_clusters)▒s_k2 Eq. 0.28\nWhich in the 1D case yields:\nχ_(d=1) (p)=(1+p)/(1−p) Eq. 0.29\n\n\n\nFig 0.6 - rough plot of average cluster size over probability - note how it diverges as p→∞ representing an approach to infinite clusters for p=1\n\n\nFinally it is also useful to note a “correlation function” g(r_i,r_j) which describes the probability of any two cells belonging to the same cluster which relates to the average size:\n∑_(r_i)▒〖g(r_i,r_j)〗=χ(p) Eq. 0.30\n\n\n\nGiven these tools of a 1D percolation case, it is possible to expand to 2D in order to examine these phase transitions a bit more closely.\nParticularly it is useful to characterise phase transitions in terms of order parameters and critical exponents.\nAs a general rule, the “order parameter” represents the n’th derivative of the system’s helmholtz free energy where a discontinuity appears; in the case of percolation, that is the probability of percolation p_∞.\nSuch order parameters can ~ generally be expressed in terms of critical exponents near the critical point:\np_∞∝|├ ​p−p_c ┤|^β Eq 0.31\nWhere β is the critical exponent that characterises this phase transition. In a 2D percolation lattice, it has been found to a value β=5/36\nThe average cluster size also experiences such a discontinuity so may be expressed in terms of its own exponent:\nχ(p)∝|├ ​p−p_c ┤|^(−γ) Eq 0.32\nWith a 2D percolation lattice having an exponent γ=43/18.\n\n\n\nNote that a lot of effects are size dependent; however generally it is useful to extract these effects for an infinite lattice.\nThe 1D lattice in fig. 0.4 may illustrate this. Consider the boundary vs “bulk” cells. The bulk cells have 2 neighbors whilst the boundary cells only have one. As such, the bulk cells are more likely to be part of a cluster than the boundary ones. This noticeably distorts the effects of the system. However this effect becomes more negligeable as the lattice grows, with the number of bulk cells growing faster than the number of boundary cells.\nSuch finite lattice effects do however need to be accounted for, as they reduce the apparent discontinuity in phase transitions, making measurement of the critical exponents more difficult.",
    "crumbs": [
      "Wk0",
      "Wk0.5 Background theory and concepts"
    ]
  },
  {
    "objectID": "Wk0/0_0_Wk0_preface.html",
    "href": "Wk0/0_0_Wk0_preface.html",
    "title": "Wk0 preface",
    "section": "",
    "text": "Wk0 preface\nThis Wk0 section is intended as a “miscelaneous” section to this lab book which contains:\n\nGeneral testing - figuring out how to use onenote features to write this labbook\nBibliography\nGeneral pre-lab prep\nGeneral lab info\n\nAs such it’s a mix of more administrative information and pre-lab preparation.",
    "crumbs": [
      "Wk0",
      "Wk0 Preface"
    ]
  },
  {
    "objectID": "Wk2_29_01_2026/2_2_additionalReading.html",
    "href": "Wk2_29_01_2026/2_2_additionalReading.html",
    "title": "Wk 2.2 Additional reading",
    "section": "",
    "text": "During Wk1 I worked on the code basics for the simulatio whilst my lab partner Scarlett produced a robust literature review on the topic. This page is intended to summarise and notate the findings of that and similar readings.\n\n\n\nScarlett wrote a quick summary of her theoretical findings which are renotated here.\nShe focused on characterising clusters in the Ising model, so outlined the following cluster observables:\n\nCluster size distribution\nCluster shape\nSpatial correlations\nTime autocorrelation\nConnection between clusters and correlations.\n\n\n\n\\(z(n) = \\frac{N_{cluster}(n)}{N}\\)                   Eq 2.1 Cluster size density $N_{cluster}(n) = $ Number of clusters of size \\(n\\) $N = $ number of spin states in ising model\n\\(z(n)\\) represents the “density” of clusters of a given size n in an ising model. Mean cluster size follows:\n\\(&lt;n&gt;_{cluster} = \\frac{\\sum n^{2}z(n)}{\\sum nz(n)}\\)                    Eq 2.2 Average cluster size\nWhich apparently has behaviour analagous to magnetic susceptibility The number of clusters per lattice point:\n\\(&lt;C&gt;_{site} = \\sum z(n)\\)                    Eq 2.3 number of clusters per site\nWith the largest cluster fraction acting as an order parameter:\n\\(f_{max} = \\frac{n_{max}}{N}\\)                   Eq 2.4\nNote that at critical temperature, \\(z(n)\\) approaches:\n\\(z(n) \\approx \\frac{1}{n^{2+\\frac{1}{\\delta}}}\\)                   Eq 2.5 Critical cluster density $= $ critical exponent of magnetisation-field response (2D case has \\(\\delta = 15\\))\n\n\n\nIt is useful to define a “cyclomatic number” for any cluster :\n\\(c=l-n+1\\)                    Eq 2.6 cyclomatic number $n = $ number of spins in a cluster $l = $ number of bonds between spins in a cluster\nGiven this, some idea of “compactness” may be defined and illustrated:\n\\(A=\\frac{c}{(n-1)^2}\\)                    Eq 2.7 Coefficient of compactness\n\n\n\nFig 2.2 - Illustration of a non compact vs compact cluster\n\n\n\n\n\nSpin correlations are described by:\n\\(C(r)=&lt;s(0)s(r)&gt;=\\frac{1}{N}\\sum s_{i} s_{i+r}\\)                   Eq 2.8 equal time correlation function\nAt non critical temperatures this experiences rapid decay:\n\\(c(r) \\approx e^{\\frac{-r}{\\zeta}}\\)                   Eq 2.9 Non critical correlation $= $ correlation length\n\\(\\zeta\\) is related to criticality, so as the model approaches criticality it grows, increasing the correlations between distant lattice points.\n\n\n\nTime autocorrelation is analagous to spatial correlations as Eq 2.7, but across some time interval \\(\\tau\\) instead of space \\(r\\):\n\\(A(\\tau) = \\frac{&lt;(M_{t}-\\overline{M})(M_{t+\\tau}-\\overline{M})&gt;}{(M_{t}-\\overline{M})^{2}}\\)                   Eq 2.10\nand typically decays as:\n\\(A(\\tau) \\approx e^{\\frac{-tau}{\\tau_{c}}}\\)                   Eq 2.11\n\n\n\nSpin correlations and cluster connectivity are directly related as:\n\\(&lt;s_{i}s_{j}&gt;=P(i&lt;-&gt;j)\\)                   Eq 2.12 spin correlations and probability of connection between two points\n\n\n\n\nScarlett found 4 Critical exponent related papers which i need to go through and summarise.\n\n\n\n\n\ncomputed crit exp \\(\\alpha, \\beta, \\gamma\\) for 3D IM\ntrained deep learning alg on IM results to reduce necessary information of system\n\n\n\n\n\n\n\n\nCalculated critical exponents on “fractal lattice” w/\\(d=1.792\\)\nused “higher order tensor renormalisation” to compute this\n\n\n\n\n\n\n\n\nfound crit exp for correlation length, specific heat, susceptibility, disconected susceptibility, magnetisation\n3D model\nused finite size scaling to find\nseems to be most “general”\n\n\n\n\nUsed a MC 3D ising model to generate a set of “sample models” and associated “replica models” over a range of temperatures. The replica models were used to decide when a model had equilibrated by comparing the magnetisations of 2 models; one which started all spin up, one which started all spin downs. When the magnetisations entered each other’s uncertainties; equilibration was determined.\nEach sample/replica measured:\n\n\\(&lt;M&gt; =\\) average magnetisation\n$ &lt;M^2&gt;$\n\\(&lt;E&gt; =\\) average energy\n\\(&lt;E^2&gt;\\)\n\nWhich yield the following observables:\n\\(C_{ave} = N\\frac{(&lt;E^2&gt; - &lt;E&gt;^2)}{T^2}\\)                   Eq 2.13 average specific heat capacity\n\\(m_{ave}=|&lt;M&gt;|\\)                    Eq 2.14 Average magnetisation\n\\(\\chi_{ave} = N\\frac{(&lt;M^2&gt; - &lt;M&gt;^2)}{T}\\)                   Eq 2.15 Average susceptibility ee \\(\\chi_{dis} = N&lt;M&gt;^2\\)                   Eq 2.16\nWhich relate to their critical exponents as:\n\\(t = T - T_c\\)                   Eq 2.17 Temperature relative to critical temperature \\(T_c\\)\n\\(T^2C_{ave} = L^{\\frac{\\alpha}{v}} \\overline{C}(tL^{\\frac{1}{v}})\\)                    Eq 2.18\n\\(m_{ave} = L^{\\frac{\\beta}{v}}\\overline{m}(tL^{\\frac{1}{v}})\\)                    Eq 2.19\n\\(T\\chi_{ave} = L^{2-\\eta}\\overline{\\chi}(tL^{\\frac{1}{v}})\\)                    Eq 2.20\n\\(\\chi_{dis} = L^{\\eta - \\overline{eta}}\\chi_{dis}(tL^{\\frac{1}{v}})\\)                    Eq 2.21\n\\(\\alpha =\\) Specific heat critical exponent\n\\(\\beta =\\) order parameter exponent\n\\(v =\\) Correlation length exponent\n\\(\\eta, \\overline{\\eta}\\) describe “power law decay”\n\\(\\gamma = (2-\\eta)v =\\) susceptibility exponent\n\\(\\overline{C}, \\overline{m}, \\overline{\\chi_{ave}}, \\overline{\\chi_{dis}}\\) are scaling functions for their respective observables.\nThe paper found the critical exponents for \\(C_{ave}\\) by acknowledging that \\(\\overline{C}\\) has a maxima \\(x_{0} = (T_{0}-T_{c})L^{\\frac{2}{v}}\\) for some \\(T_{0}\\) where \\(T^{2}C_{ave}\\) is maximised; so by finding \\(T_{0}(L)\\) over a range of \\(L\\), the exponent \\(v\\) and critical temperture can be fitted with:\n\\(T_{0}(L) = x_{0}L^{\\frac{-1}{v}} + T_{c}\\)                    Eq 2.21\nSo the free parameters \\(x_{0}\\), \\(v\\) and \\(T_{c}\\) can be found by polynomial fit.\nThe paper implies similar procedure may be used for the other observable parameters.\nThe paper then discusses these exponents in a wider context that i’m not fully versed in, and at the moment not fully interested in. The outline in measuring critical exponents was the main point of interest for this lab at this point.\n\n\n\n\n\n\n\nexplanation of mean field theory approx",
    "crumbs": [
      "Wk2",
      "Wk 2.2 Additional Reading"
    ]
  },
  {
    "objectID": "Wk2_29_01_2026/2_2_additionalReading.html#foreword",
    "href": "Wk2_29_01_2026/2_2_additionalReading.html#foreword",
    "title": "Wk 2.2 Additional reading",
    "section": "",
    "text": "During Wk1 I worked on the code basics for the simulatio whilst my lab partner Scarlett produced a robust literature review on the topic. This page is intended to summarise and notate the findings of that and similar readings.",
    "crumbs": [
      "Wk2",
      "Wk 2.2 Additional Reading"
    ]
  },
  {
    "objectID": "Wk2_29_01_2026/2_2_additionalReading.html#cluster-theoretical-overview-overview-provided-by-scarlett-rewritten-here-for-convenience",
    "href": "Wk2_29_01_2026/2_2_additionalReading.html#cluster-theoretical-overview-overview-provided-by-scarlett-rewritten-here-for-convenience",
    "title": "Wk 2.2 Additional reading",
    "section": "",
    "text": "Scarlett wrote a quick summary of her theoretical findings which are renotated here.\nShe focused on characterising clusters in the Ising model, so outlined the following cluster observables:\n\nCluster size distribution\nCluster shape\nSpatial correlations\nTime autocorrelation\nConnection between clusters and correlations.\n\n\n\n\\(z(n) = \\frac{N_{cluster}(n)}{N}\\)                   Eq 2.1 Cluster size density $N_{cluster}(n) = $ Number of clusters of size \\(n\\) $N = $ number of spin states in ising model\n\\(z(n)\\) represents the “density” of clusters of a given size n in an ising model. Mean cluster size follows:\n\\(&lt;n&gt;_{cluster} = \\frac{\\sum n^{2}z(n)}{\\sum nz(n)}\\)                    Eq 2.2 Average cluster size\nWhich apparently has behaviour analagous to magnetic susceptibility The number of clusters per lattice point:\n\\(&lt;C&gt;_{site} = \\sum z(n)\\)                    Eq 2.3 number of clusters per site\nWith the largest cluster fraction acting as an order parameter:\n\\(f_{max} = \\frac{n_{max}}{N}\\)                   Eq 2.4\nNote that at critical temperature, \\(z(n)\\) approaches:\n\\(z(n) \\approx \\frac{1}{n^{2+\\frac{1}{\\delta}}}\\)                   Eq 2.5 Critical cluster density $= $ critical exponent of magnetisation-field response (2D case has \\(\\delta = 15\\))\n\n\n\nIt is useful to define a “cyclomatic number” for any cluster :\n\\(c=l-n+1\\)                    Eq 2.6 cyclomatic number $n = $ number of spins in a cluster $l = $ number of bonds between spins in a cluster\nGiven this, some idea of “compactness” may be defined and illustrated:\n\\(A=\\frac{c}{(n-1)^2}\\)                    Eq 2.7 Coefficient of compactness\n\n\n\nFig 2.2 - Illustration of a non compact vs compact cluster\n\n\n\n\n\nSpin correlations are described by:\n\\(C(r)=&lt;s(0)s(r)&gt;=\\frac{1}{N}\\sum s_{i} s_{i+r}\\)                   Eq 2.8 equal time correlation function\nAt non critical temperatures this experiences rapid decay:\n\\(c(r) \\approx e^{\\frac{-r}{\\zeta}}\\)                   Eq 2.9 Non critical correlation $= $ correlation length\n\\(\\zeta\\) is related to criticality, so as the model approaches criticality it grows, increasing the correlations between distant lattice points.\n\n\n\nTime autocorrelation is analagous to spatial correlations as Eq 2.7, but across some time interval \\(\\tau\\) instead of space \\(r\\):\n\\(A(\\tau) = \\frac{&lt;(M_{t}-\\overline{M})(M_{t+\\tau}-\\overline{M})&gt;}{(M_{t}-\\overline{M})^{2}}\\)                   Eq 2.10\nand typically decays as:\n\\(A(\\tau) \\approx e^{\\frac{-tau}{\\tau_{c}}}\\)                   Eq 2.11\n\n\n\nSpin correlations and cluster connectivity are directly related as:\n\\(&lt;s_{i}s_{j}&gt;=P(i&lt;-&gt;j)\\)                   Eq 2.12 spin correlations and probability of connection between two points",
    "crumbs": [
      "Wk2",
      "Wk 2.2 Additional Reading"
    ]
  },
  {
    "objectID": "Wk2_29_01_2026/2_2_additionalReading.html#critical-exponents-papers",
    "href": "Wk2_29_01_2026/2_2_additionalReading.html#critical-exponents-papers",
    "title": "Wk 2.2 Additional reading",
    "section": "",
    "text": "Scarlett found 4 Critical exponent related papers which i need to go through and summarise.\n\n\n\n\n\ncomputed crit exp \\(\\alpha, \\beta, \\gamma\\) for 3D IM\ntrained deep learning alg on IM results to reduce necessary information of system\n\n\n\n\n\n\n\n\nCalculated critical exponents on “fractal lattice” w/\\(d=1.792\\)\nused “higher order tensor renormalisation” to compute this\n\n\n\n\n\n\n\n\nfound crit exp for correlation length, specific heat, susceptibility, disconected susceptibility, magnetisation\n3D model\nused finite size scaling to find\nseems to be most “general”\n\n\n\n\nUsed a MC 3D ising model to generate a set of “sample models” and associated “replica models” over a range of temperatures. The replica models were used to decide when a model had equilibrated by comparing the magnetisations of 2 models; one which started all spin up, one which started all spin downs. When the magnetisations entered each other’s uncertainties; equilibration was determined.\nEach sample/replica measured:\n\n\\(&lt;M&gt; =\\) average magnetisation\n$ &lt;M^2&gt;$\n\\(&lt;E&gt; =\\) average energy\n\\(&lt;E^2&gt;\\)\n\nWhich yield the following observables:\n\\(C_{ave} = N\\frac{(&lt;E^2&gt; - &lt;E&gt;^2)}{T^2}\\)                   Eq 2.13 average specific heat capacity\n\\(m_{ave}=|&lt;M&gt;|\\)                    Eq 2.14 Average magnetisation\n\\(\\chi_{ave} = N\\frac{(&lt;M^2&gt; - &lt;M&gt;^2)}{T}\\)                   Eq 2.15 Average susceptibility ee \\(\\chi_{dis} = N&lt;M&gt;^2\\)                   Eq 2.16\nWhich relate to their critical exponents as:\n\\(t = T - T_c\\)                   Eq 2.17 Temperature relative to critical temperature \\(T_c\\)\n\\(T^2C_{ave} = L^{\\frac{\\alpha}{v}} \\overline{C}(tL^{\\frac{1}{v}})\\)                    Eq 2.18\n\\(m_{ave} = L^{\\frac{\\beta}{v}}\\overline{m}(tL^{\\frac{1}{v}})\\)                    Eq 2.19\n\\(T\\chi_{ave} = L^{2-\\eta}\\overline{\\chi}(tL^{\\frac{1}{v}})\\)                    Eq 2.20\n\\(\\chi_{dis} = L^{\\eta - \\overline{eta}}\\chi_{dis}(tL^{\\frac{1}{v}})\\)                    Eq 2.21\n\\(\\alpha =\\) Specific heat critical exponent\n\\(\\beta =\\) order parameter exponent\n\\(v =\\) Correlation length exponent\n\\(\\eta, \\overline{\\eta}\\) describe “power law decay”\n\\(\\gamma = (2-\\eta)v =\\) susceptibility exponent\n\\(\\overline{C}, \\overline{m}, \\overline{\\chi_{ave}}, \\overline{\\chi_{dis}}\\) are scaling functions for their respective observables.\nThe paper found the critical exponents for \\(C_{ave}\\) by acknowledging that \\(\\overline{C}\\) has a maxima \\(x_{0} = (T_{0}-T_{c})L^{\\frac{2}{v}}\\) for some \\(T_{0}\\) where \\(T^{2}C_{ave}\\) is maximised; so by finding \\(T_{0}(L)\\) over a range of \\(L\\), the exponent \\(v\\) and critical temperture can be fitted with:\n\\(T_{0}(L) = x_{0}L^{\\frac{-1}{v}} + T_{c}\\)                    Eq 2.21\nSo the free parameters \\(x_{0}\\), \\(v\\) and \\(T_{c}\\) can be found by polynomial fit.\nThe paper implies similar procedure may be used for the other observable parameters.\nThe paper then discusses these exponents in a wider context that i’m not fully versed in, and at the moment not fully interested in. The outline in measuring critical exponents was the main point of interest for this lab at this point.\n\n\n\n\n\n\n\nexplanation of mean field theory approx",
    "crumbs": [
      "Wk2",
      "Wk 2.2 Additional Reading"
    ]
  },
  {
    "objectID": "Wk3_05_02_2026/3_1_labLog.html",
    "href": "Wk3_05_02_2026/3_1_labLog.html",
    "title": "labBook",
    "section": "",
    "text": "#Wk3 lab log",
    "crumbs": [
      "Wk3",
      "Wk 3.1 Lab log"
    ]
  },
  {
    "objectID": "Wk3_05_02_2026/3_1_labLog.html#section",
    "href": "Wk3_05_02_2026/3_1_labLog.html#section",
    "title": "labBook",
    "section": "05/02/2026:",
    "text": "05/02/2026:\nQuick group meeting w/Turci; outlined report requirements, structure and expectations\nDiscussed w/Scarlett direction; she’s more interested in cluster behaviour, i’m more interested in critical exponents. Most of the core code functionality is setup; so there may be a point at which we start to diverge in our research interests. TBD, more code experimentation and setup to do, but to be considered.\nquickly attempted to generate data over temperature; however 1/np.array doesn’t create a list of reciprocals; so that was a mistake\nam generally fiddling with data ranges and plotting graphs to get an idea of what data ranges would be ideal for this project. generally speaking range \\(\\beta \\in [0, 3]\\) seems appropriate.\n\ngeneral reading\nReading more through cite{complexityAndCriticality} whilst waiting for data to generate.\nNote to self, can incorporate boltzman distribution Eq. 0.4:\n\\(p = \\frac{e^{-H\\beta}}{Z}\\)              Eq. 0.4\nInto observable average as:\n\\(&lt;A&gt; = \\frac{1}{Z}\\sum{e^{-H\\beta} \\cdot A_{i}}\\)              Eq. 3.1\nmy be useful to better compute observables in main config Also, may be useful to normalise observables to number of spins ()\n\n\ngeneral log continued\nHave implemented above; the absolute magnetisation over beta plots change from:\n\n\n\nFig 3.1 - \\(M(\\beta)\\) before implementing Eq. 3.1 for (\\(d=2\\), \\(L=20\\))\n\n\nto\n\n\n\nFig 3.2 - \\(M(\\beta)\\) after implementing Eq. 3.1 for (\\(d=2\\), \\(L=20\\))\n\n\nThis change seems to better incorporate the probability of configs, thus “converging” the average magnetisation to the “true” magnetisation significantly faster and better. This has made the phase transition a lot clearer; in (\\(d=2\\), \\(L=20\\)) appears to be ~ \\(\\beta_{c}=0.4\\) - note however that equilibration times are a lot longer near criticality, so is noisier\n\n\nTo do list:\n\nimplemenet variable equilibration steps into dataset (so can pass in either float, or array of equilibration values) (done)\nnormalise observables\nFigure out way to compute critical \\(\\beta_{c}\\) from data\ndo a basic fit to \\(|T - T_{c}|^{x}\\) where \\(x\\) is the critical exponent\n\nexamine basic \\(x\\) over a range of sizes to establish finite size effects\ntry and figure out way to account for finite size effects\n\nrepeat analysis in higher dimensions\n\n\n\nlab log continuation\nhave tested a narrower temperature range (\\(T \\in [0.5, 5]\\)), yields:\n\n\n\nFig 3.3 - absolute magnetisation over temperature for (\\(d=2\\), \\(L=20\\))\n\n\nNote how high temperatures seemingly have a lot more uncertainty.\nAlso generated plots for the other observables:\n\n\n\nFig 3.4 - energies (\\(d=2\\), \\(L=20\\))\n\n\n\n\n\nFig 3.5 - partition function (\\(d=2\\), \\(L=20\\))\n\n\n\n\n\nFig 3.6 - entropies (\\(d=2\\), \\(L=20\\))\n\n\nNote how Fig 3.4 seems to have some asymmetry in it’s transition. It appears to have a near discontinuous increase about \\(T\\approx2\\), but has a smoother continuous change as T increases. A few potential explanations worth considering:\n\nFrom cite{complexityAndCriticality}, it seems that the \\(|T - T_{c}|^{x}\\) expressions are usually one sided; this may be a manifestation of that?\nMay be finite size effect\n\nhemlholtz plot finally produced (code errors required re-simulating data for useful results):\n\n\n\nFig 3.7 - Helmholtz free energies (\\(d=2\\), \\(L=20\\))\n\n\nIt seems that the critical temperature coincides w/minima of helmholtz free energy (ignoring outlying data points). Unsure if this is coincidental or not, however if so, may provide an incredibly easy method of finding the critical temperature.\n\ndataset data analysis:\nStarting to think about how to analyse the data being generated. Particularly, need to do following:\n\nfind the critical temperature\nfit exponent curves of form \\(|T - T_{c}|^{x}\\) to observables as appropriate (they may be one sided)\n\nTo do so however it is necessary to filter for evident outliers in the data. This may be done manually, however it’d be more convenient to do so automatically. A potential means to do this would be to filter by a generic “continuity check”. Given a particular observable \\(a_{i}\\) with neighbours \\(a_{i-1}\\), \\(a_{i+1}\\); it would interpolate between the neighbours; if \\(a_{i}\\) is within uncertainty of the interpolated value, it is accepted, else it is rejected. This approach may cause erroneous results however when evaluating the edges of the dataset. Since this method relies on neighbours, it can’t validate data at either end of the dataset. It may be possible to extrapolate to the data edges from the next 2 nearest points?\nThat may be a better way of doing it; extrapolate from the 2 nearest points for the continuity check. For most datapoints that will be both neighbours, however for the endpoints it will extrapolate from the data.\nThis method may be a bit crude, but should discard most outliers. THe main risk is that it may “flatten” and trends in the data due to datapoints being too far apart from each other. However adequate sampling density should avoid this issue.\nQuickly implementing this method on the helmholtz energies yields: \nWhich seems to not work. minor fixes; code is UGLY, but it seems to work. Filtering seems to work.\nTalked to Dr Turci, asked about the apparent minima in helmholtz free energy; pointed out its a non standard way of finding critical temps; suscpetibility/spin and specific capacity/spin are more common:\n\\(\\chi = \\beta\\frac{&lt;M&gt;^2 - &lt;M^{2}&gt;}{N}\\)             Eq. 3.2\n\\(C = \\beta^{2} \\cdot k_{b} \\cdot \\frac{&lt;E&gt;^2 - &lt;E^{2}&gt;}{N}\\)             Eq. 3.3\nattempted implementation; doesn’t seem to be working yet. needing to resimulate the data. Am also unsure how to propagate errors through the susceptibility and heat capacity either. propagating errors in terms yields:\n\\(\\Delta \\chi^{2} = (\\frac{\\beta}{N})^{2}((2&lt;M&gt;)^{2}\\Delta&lt;M&gt;^2 + \\Delta&lt;M^{2}&gt;^2)\\)             Eq. 3.4\n\\(\\Delta C^{2} = (\\frac{\\beta^{2}}{N})^{2}((2&lt;E&gt;)^{2}\\Delta&lt;E&gt;^2 + \\Delta&lt;E^{2}&gt;^2)\\)             Eq. 3.5\nsusceptibility seems to be working-ish (errors are massive but given expression i’m not sure error propagation makes sense as a concept) - also peak of susceptibility seems to be offset relative to apparent critical temperature in all other graphs (\\(T_Cs \\approx 3\\)).\nError found. Eqs 3.2-3.5 use the mean, code was using weighted average instead. Quick graphs produced:\n\n\n\nFig 3.8 - Susceptibility (\\(d=2\\), \\(L=20\\))\n\n\n\n\n\nFig 3.9 - Heat capacity (\\(d=2\\), \\(L=20\\))\n\n\nDon’t think these work fully. actually unsure if i should used weighted average; textbook seems to indicate weighted average? All of these tests have been conducted on a single dataset (saved for convenience) - it may be worth testing on a larger, more fine grain dataset.\n\n\nTo Do list:\n\nfix susceptibility and heat capacity computations\nrun big dataset; see if that fixes\nfigure out exponent fitting\nadd a “recompute observables” method to config object; that way old config objects can be updated if computations change\ndouble check all observable computations",
    "crumbs": [
      "Wk3",
      "Wk 3.1 Lab log"
    ]
  },
  {
    "objectID": "Wk3_05_02_2026/3_1_labLog.html#section-1",
    "href": "Wk3_05_02_2026/3_1_labLog.html#section-1",
    "title": "labBook",
    "section": "06/02/2026:",
    "text": "06/02/2026:\nNot doing proper lab work today, but am generating some larger datasets in the background (40 configs over \\(T\\in [0.5, 5]\\) for 100x100, 1000 Equil steps, 500 sim steps) to have better testing/development data. Furthermore, Dr Turci did some more thinking on the apparent minimal helmholtz free energy cooresponding to critical temperature. Apparently the critical temperture should show up in the second derivative \\(\\frac{d^{2}F}{dT^{2}}\\), but shouldn’t be visible as is (given \\(dF = pdV -SdT\\), and entropy SHOULD be positive (may need to double check thats computed correctly), F should decrease over data range); Dr Turci suggests it may be poor sampling about \\(T_{c}\\); i suspect i’ve computed an observable incorrectly.",
    "crumbs": [
      "Wk3",
      "Wk 3.1 Lab log"
    ]
  },
  {
    "objectID": "Wk3_05_02_2026/3_1_labLog.html#section-2",
    "href": "Wk3_05_02_2026/3_1_labLog.html#section-2",
    "title": "labBook",
    "section": "07/02/2026:",
    "text": "07/02/2026:\n\nlog:\nobservables implemented:\n\nEnergy (code verified)\nMagnetisation (code verified)\nPartition function (code verified)\nEntropy (code verified)\nHelmholtz free energy (code verified)\nBoltzman weights (Given average observables are weighted by boltzman distribution, computing these weights seperatedly made sense) (code verified)\nSusceptibility (code verified)\nHeat capacity (code verified)\n\nquickly rewrote calcEnergy - needed to be generalised; plus was opportunity to double check correct implementation Similarly rewrote partition function method as it used a very similar implementation to calcEnergy\nHave double checked all the computation; nothing is immediately wrong reran data; seems susceptibility fixed itself, however heat capacity hasn’t\nI suspect it may be due to noise in the data? would need to run a larger dataset to verify.\nImplementing “continuous saving” to “fromBetas” dataset construction - ensures that data saves over dataset generation ensuring interrupted runs don’t wipe out whole chunk of data (also implemented “already exists” checks as part of this, allows for quick restarts) - code is messy but works and shouldn’t need to be touched significantly\ncleaned up code a bit\nhave started working in critical exponent fitting.\ncomparing values to https://www.phys.ens.psl.eu/~jacobsen/AIMES/Ising.pdf which quotes \\(\\beta=\\frac{1}{8}\\); testing finds \\(\\beta=0.03\\) for 20x20 config; off by a factor of 4. Not the worst result however, but the fit leaves much to be desired:\n\n\n\nFig 3.10 - Fit attempt to \\(M\\propto |T-T_{c}|^{\\beta}\\) \\(\\beta = -0.03\\pm0.05\\) (\\(d=2\\), \\(L=20\\))\n\n\nWill need to do some debugging to see where this may be coming from…\nhave created new .py file; simply a place to streamline “bulk data generation” - imports everything else, that way desired datasets can be neatly laid out as a queue Furthermore, have updated how datasets save configs to files; they will also check if duplicate configs already exist and skip them if they do. This makes restarting interrupted datasets easier, and means that its easier to add data to an existing dataset (I.E, it may be useful to add a high density region around critical points with higher equilibration steps).\nAm setting my laptop to generate some main data runs overnight; these are all 2D models in 3 batches which represent 1000, 2000, 3000 equilibration steps. The first 2 batckes (1000, 2000 steps) have a range of sizes [20, 30, 40] whilst the final batch has a range [20, 30, 40, 50, 60, 70, 80, 90]. This was done to compare the effects of equilibration times, as well as to begin investigating finite size effects empirically. The longest equilibration dataset has more sizes because it is expected to be the highest quality data, and establishing the effect of equilibration can be done with a more limited subset of data.",
    "crumbs": [
      "Wk3",
      "Wk 3.1 Lab log"
    ]
  },
  {
    "objectID": "Wk3_05_02_2026/3_1_labLog.html#section-3",
    "href": "Wk3_05_02_2026/3_1_labLog.html#section-3",
    "title": "labBook",
    "section": "09/02/2026",
    "text": "09/02/2026\nLaptop is still generating data. First night, it shut itself down interrupting progress. Second night it was kept on, and mostly handled the 100x100 over 3000eq steps, 500sim steps. Starting at ~ 14:00 08/02/2026, till now (10:00 09/02/2026) it has generated 18/40 datapoints. Given that each simulation calls “mcMove” for each step taken, and that “mcMove” loops through and performs a computation for each of its cells; the above timings gives a rough computation speed of \\(8750 cell computations/s\\) (alternatively \\(31.5 \\cdot 10^{6} cell computations/Hr\\) or \\(0.756 \\cdot 10^{9} cell computations/day\\)). THis is useful as it allows for time estimates for future datasets to be planned in advance, allowing for better planning. The theoretical timings for potential datasets of 3000eq, 500sim steps, 40 datapoints follow:\n\n3D \\(20^{3}\\): ~1.48 days\n3D \\(22^{3}\\): ~1.97 days\n4D \\(10^{4}\\): ~ 1.85 days\n5D \\(6^{5}\\): ~1.44 days\n6D \\(4^{6}\\): ~0.76 days\n4D \\(20^{4}\\): ~ 29.6 days\n\nThe large 4D example is impractical to generate given the time constraints of this project. However all the other options aren’t impractical. Combined they’d take ~1Wk non stop computation to complete. Whilst there are likely to be some interruptions, i think this may be feasible. The main question is to what degree can finite size effects be accounted for in such small model sizes; if its possible to account and “correct” for such finite size effects, comparing critical exponents over \\(d\\in[2,6]\\) may be interesting. However given the significant computational investement, its worth knowing the following:\n\nHow to correct/account for finite size effects - this is crucial, otherwise the different sized different dimensioned datasets can’t be compared to each other\nWhat should the dataset parameters be for arbitrary dimension/size:\n\nHow does model size and dimension relate to required equilibration steps\nHow does critical temperature relate to dimensionality\nthis is important because of the heavy computational investment; exploring a particular dimension to determine these parameters experimentally would be a waste of time; so trying to estimate the appropriate equilibration steps and temperature ranges.\n\n\nAs such, its worth doing some additional reading/research particularly to address correcting for finite size effects.\n\nminor reading\ncite{complexityAndCriticality} has a section on scaling laws (p188 onwards); the key insight worth taking away is normalising the temperature. The critical exponents for a given observable typically look like:\n\\(O \\propto |T-T_{c}|^{\\beta}\\)\nthe insight is to normalise \\(T\\) as:\n\\(t=\\frac{T-T{c}}{T_{c}}\\)              Eq. 3.6\ns.t that the exponent expression become:\n\\(O \\propto |t|^{\\beta}\\)              Eq. 3.7\n\\(O = h(x) |t|^{\\beta}\\)              Eq. 3.8\nwhere \\(h(x)\\) represents some scaling function. The book goes into further detail as to what these scaling functions should look like, however given the data being generated, it should be possible to fit for \\(h(x)\\) and \\(\\beta\\) empirically. It may be interesting to compare the empirically fit \\(h(x)\\) to analytical solutions of time allows, but it seems that its possible to entirely bypass that analytical component (which makes this project more straightofrward for the moment).\nNote the book goes into some detail about the scaling function. Examining the scaling function for magnetisation specifically, it generally has the shape:\n\n\n\nFig 3.11 - Scaling function for magnetisation rough sketch - note \\(M(\\frac{h}{|t|^{\\Delta}})\\) as a function of external magnetic field\n\n\nAnd relationships:\n\\(lim_{x\\to 0}M_{+}(x)=0\\)              Eq. 3.9\n\\(lim_{x\\to 0}M_{-}(x)=c \\ne 0\\)              Eq. 3.10\nSo when fitting the data, it will be necessary to consider which side of the critical point is being evaluated\n\n\nfurther reading cite{scalingCardy}\nThe textbook cite{scalingCardy} has a chapter going over group renormalisation methods. I don’t fully understand the concept, but will attempt to summarise the useful parts of it here.\n\nrenormalisation transform\nThe key idea is the idea of a “block” or “renormalisation transform”. This entails “coarsening” the ising model, grouping spins together into larger “aggregate” spins as illustrated:\n\n\n\nIllustration of block transform\n\n\nThis transform is noted as:\n\\(s' = R/{s/}\\)              Eq. 3.11\nThis has the effect of rescaling the model to different sizes. Crucially for later, behaviour at the critical point is unchanged by renormalisation. Likewise, the partition function is generally unchanged by renormalisation:\n\\(Z' = R{Z}\\)\n\\(\\downarrow\\)\n\\(Z'=Z\\)              Eq. 3.12\n\n\nrenormalisation of coupling strength \\(J\\) and example of correlation lenght\nA particular example noted by the book, is the renormalisation of the coupling strength \\(J\\) (the book uses \\(K\\)) in the ising hamiltonian, which follows:\n\\(R{J} = J'  \\approx b^{d-1}J\\)              Eq. 3.13\nwhere \\(b\\) is an arbitrary parameter, \\(d\\) is the dimension. Furthermore (apparently), for \\(J\\) close to the critical point %J_{c}%, the following relation follows:\n\\(J' \\approx J_{c}+b^{y}(J-J_{c})\\)              Eq. 3.14\n\\(y=\\frac{ln(J_{c})}{ln(b)}\\)              Eq. 3.15\nFor some \\(J\\) dependent observable (the book uses correlation length \\(\\xi(J)\\)), the renormalisation transform results in:\n\\(\\xi(J)=b\\xi(J)\\)              Eq. 3.16\nAnd given some \\(n(J)\\) repeated applications of the transform, this becomes:\n\\(\\xi(J)=\\xi_{0}b^{n(J)}\\)              Eq. 3.17\nGiven the critical behaviour of \\(\\xi\\propto (J-J_{c})^{-v}\\), it is possible to substitute in Eq 3.17 as:\n\\(\\xi \\propto (b^{y}(J-J_{c}))^{-v}\\)              Eq. 3.18\nwhich generates a relationship between the critical exponent \\(v\\) and the parameter \\(y\\) derived from the renormalisation transform (I’m unsure how to do this however).\n\n\ngeneral theory - group eq\nMore generally, the renormalisation transform on some observable \\(K\\) can be expressed as a linear transform of form:\n\\(K'_{a}-K_{c a} = \\sum_{b}T_{ab}(K_{b}-K_{c b})\\)              Eq. 3.19\n\\(T_{ab} = frac{dK'_{a}}{dK_{b}}|_{k=k_{c}}\\)              Eq. 3.20\nSkipping over a bunch of maths “scaling variables” can be defined and expressed as:\n\\(u_{i} = \\sum \\phi_{a}^{i}(K_{b}-K_{c b})\\)              Eq. 3.21\n\\(u_{i} = b^{y_{1}}u_{i}\\)              Eq. 3.22\nwhere \\(y_{i}\\) represent the renormalisation group eigenvalues (although the implications of this are lost on me).\nNote that these eigenvalues are classified as “relevant” if \\(y_{i}&gt;0\\).\n\n\nfree energy scaling\nGiven this theory, it is possible to rescale the Helmholtz free energy as:\n\\(f(\\{K\\}) = g(\\{k\\})+b^{-d}f(\\{K'\\})\\)              Eq. 3.23\n\\(b\\) is some constant and \\(d\\) is the dimensionality. where \\(g({K})\\) represents an inhomogeneous term which can be neglected when looking for critical exponents, so Eq. 3.23 becomes:\n\\(f(\\{K\\}) = b^{-d}f(\\{K'\\})\\)              Eq. 3.23\nExpressing this in terms of thermal and magnetic scaling parameters \\(u_{t}\\), \\(u_{h}\\) eventually yields:\n\\(f_{s}(t,h) = |\\frac{t}{t_{0}}|^{\\frac{d}{y_{t}}}\\Phi(\\frac{\\frac{h}{h_{0}}}{(\\frac{t}{t_{0}})^{\\frac{y_{h}}{y_{t}}}})\\)              Eq. 3.24\nwhere \\(t_{0}, h_{0}\\) are scaling constants (unsure how to find these generally, may be relative to reference model size?)\nFurther note i’m pretty sure this is in terms of normalised temperature \\(t=\\frac{T-T_{c}}{T_{c}}\\), but not fully sure.\n\n\nrelationship to critical exponents\nGiven the free energy Eq 3.24, critical exponents for all other quantities follow:\n\nHeat capacity\n\\(C=\\frac{d^2f}{dt^{2}}\\propto |t|^{-\\alpha}\\)\n\\(\\alpha = 2-\\frac{d}{y_{t}}\\)\n\n\nSpontaneous magnetisation\n\\(m=\\frac{df}{dh}\\propto (t)^{\\beta}\\)\n\\(\\beta=\\frac{d-y_{h}}{y_{t}}\\)\n\n\nSusceptibility\n\\(\\chi=\\frac{d^2f}{dh^{2}}\\propto |t|^{-\\gamma}\\)\n\\(\\gamma = \\frac{2y_{h}-d}{y_{t}}\\)\n\n\nmagnetisation\n\\(m=\\propto (h)^{\\frac{1}{\\delta}}\\)\n\\(\\delta=\\frac{y_{h}}{d-y_{h}}\\)\nAll of these relate to each other with the relations:\n\\(\\alpha + 2\\beta +\\gamma = 2\\)             Eq. 3.25\n\\(\\alpha + \\beta(1+\\delta)=2\\)             Eq. 3.26\n\n\n\n\nplans from this point forwards:\nAs it stands, I think i have a better idea of the renormalisation theory, however an still somewhat unsure how to use it. Particularly i’m unsure how to find the \\(y_{t}\\), \\(y_{h}\\) scaling factors (i’m assuming that its scaling factors relative to a “baseline” size? so y_{0} would be value for baseline size, muliplied by y_{t} for any other size?).\nAs such I have 2 options to deal with critical exponents:\n\nFit to \\(t=\\frac{T-T_{c}}{T_{c}}\\) instead of T - this may already solve my issues in fig. 3.10\nAsk Dr Turci how to relate current understanding of renormalisation theory to the data i have\n\nThere are 2 possible outcomes of that:\n\nI can effectively renormalise the results of any sized configuration to an equivilant “standard size”\nI can’t do that\n\nIf its possible to renormalise the results, it becomes possible to measure critical exponents and compare them between differently sized models. If this is the case, it may be possible to examine the exponents of significantly higher dimensions whilst simulating relatively small sizes. If so, it’ll be necessary to setup a queue of desired higher dimensioned datasets as soon as possible, and run it near continuously. If not i may need to restrict my investigations to \\(d=2,3\\) for size \\(L=20\\).\nAt any rate, the next coding objective is to figure out the exponent expression fitting and create a robust method for measuring the critical exponents for all observables (heat capacity, magnetisation, susceptibility).\nI’ll also need to find further references for: * values for critical exponents (wikipedia has an uncited table which is annoying) *",
    "crumbs": [
      "Wk3",
      "Wk 3.1 Lab log"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "labBook",
    "section": "",
    "text": "This is a digital lab book for Y3 lab module produced using quarto."
  },
  {
    "objectID": "Wk1_22_01_2026/1_1_labLog.html",
    "href": "Wk1_22_01_2026/1_1_labLog.html",
    "title": "1.1 lab session notes",
    "section": "",
    "text": "Lab induction\nDr Turci walks us through introduction\nDiscuss refactoring code to give us a “complete virtual lab apparatus”:\nA complete system which we can use to generate arbitrary configuration datasets for study\nSetup github repo\nStart refactoring/writing code (hopefully finish this quickly - that way rest of lab can be running simulations w/minimal need for more coding)\n\nReasoning for this:\n\nWhat is needed from code seems fairly clear at this point; need to be able to generate arbitrary configurations simulated to equilibration; need to be able to extract arbitrary observables; need to be able to assemble these into datasets over parameter variation\nCurrently still going through “percolation” chapter - understanding of theory isn’t fully there yet, however understanding of code requirements ~ is - therefore start with what is known, and familiarise self with theory more later\n\n\n\n\nTurci introduced us to project: * Gave us overview\nMet up w/lab partner, are trying to figure out project direction and logistical questions: * Setup shared google drive folder to share materials * Setup planning document * General starting aims: * Find critical exponents * Study cluster behavior * Vary over temperature and dimensionality * Agreed to refactor code in terms of config object * Object oriented config object w/pickle library save/load functionality - given that sims may be computationally intensive, saving results for future reanalysis may save compute time * Decided to use inheritance structure; parent config, and 1D, 2D, 3D, 4D config children - sim steps will look different in different dimensions and may be difficult to generalize - hence inheritance structure\nTalked to Turci: * Need to figure out when state converges\n\nSetup code refactoring thus far:\n\nParent config class - has saving/load methods\nSetup 2D config as test child case\n\n\nHave setup code in object oriented approach: Abstract config class 2D config class Have done rudimentary testing\nTo do list: * FIX STATE LIST\n\nClean up code\nImplement higher and lower dimensions classes\nRead up and figure out critical exponents\nOptimisation and factorisation:\n\nFigure out numba python library; speed up system\nMaybe rework observable lists into pandas dataframe? May be more generalisable/easier to work with\nDouble check observable calculations\nAdd “average observables” - need to be able to average over several iterations\nMaybe save multiple config steps?\n\nPotentially setup “dataset” object? Set of config objects along some varied quantity? Save to folder of\nSetup github quarto lab book; switch over to lab book\n\n##Lab log (24/01/2026):\n\nDecided to switch back to original state system for now; ik it works; may need to refactor it later\nGonna try and implement numba on 2D case; test everything in 2D, then can expand out to higher dimensions\nNvm; gonna try and get good bones in, and save all states\n\nFor current use, two general test cases:\n\nstandardTest2000 = Ising model 100x100 cells run for 2000 steps\nstandardTest500 = Ising model 100x100 cells run for 500 steps\nHave implemented saving all states - it does make the saved files significantly larger (I.E, 100x100 model w/100 steps, saving energy/magnetization observables is 32.5Mb) - quick test standardTest2000 s; took several minutes (didn’t time it, but took a while), saved to 192.5 Mb\n\nCurrently that is acceptable; (given 100x100x2000 = 2x10^7 stored cells)) that suggests each cell uses ~20 bytes of storage space; so extrapolating to a 3D case 100^3⋅2000=40Gb - a 4D case may be prohibitive w/~4Tb storage space for the same size;\n\nCould add discard functionality (I.E, don’t need the initial equilibration steps)\nCould only save the observables over that simulation length\nHowever for now having the default “save all raw data” option is good to have\n\n\nThinking of optimisations:\n\nmcMove method\ncalcEnergy method\nBoth are VERY inefficient; in similar ways\nBoth run through entire configuration; both check adjacent cells; currently implemented through for loops; however some steps may be more efficiently done by either numpy array operations, or numba implementations\nIt may also be interesting to see if adjacency checks could be generalised/factorised to any dimension; if that’s possible, inheritance structure becomes unecessary, as the algorithm could be implemented for all dimensionalities\n\nIt would be very useful if generalising is possible; this would allow detailed study of higher dimensional ising models\n\nConfig state storage - currently an array of integer values; am wondering if can switch to a binary representation - would make storage of saved files more efficient (I.E, 4D 100x100x2000 case could optimistically be reduced to 200Gb)\nData discard system - options to discard all but the N most recent configurations - storage data optimisation\n\n\nIf its possible to generalise the dinemsionality of the model; that opens up more lines of investigation\nUnsure which optimisations are worth it. Plan of action:\n\nTry and get numba working\nFamiliarise self w/numba operations\nTry and refactor/optimise above with numba in mind\n\nGot numba working; doing rudimentary tests to see if it improves performance:\n\nAdding numba decorator to mcMove method\nstandardTest500 yields Dt = 96.8 s\nstandardTest500 w/out numba yields: 92.2 s\nThis suggests no significant speedup\n\nSuggests I’m using it incorrectly\n\nTesting standardTest500 w/ provided code;\n\nw/out numba: 38.8 s\nW/ numba: 4.26 s\n\nMaybe object oriented approach is breaking stuff?\n\nChanged mc move; moved all self checks to runSimulation instead; that way mcMove is more “anonymous” and doesn’t need to call self\n\nstandardTest500: 47.8 s\nSignificant improvement\n\nWondering if difference between provided code and own code is due to “additional” code run by “simulationRun” - have commented out plotting and appending observables\n\nstandardTest500: 7.3 s\n\n\nTakeaways:\n\nSince Numba compiles individual functions to machine code; it is advisable for numba accelerated functions/methods to be “streamlined” w/out needing to refer to non accelerated code\n\nUnsure if numba accelerated functions can refer to other accelerated functions\n\nTest case: adjacency code in mcMove; move into separate accelerated method\n\nCan move it to separate function, but numba gets confused if it’s a class method\nstandardTest500: 6.9 s\n\n\n\nNumba seems to work; moving onto some architecture changes: * Current plan is to have inheritance structure w/specific dimension configs hard coded * 3 main differences between dimensions: Dimension of array First initial state Adjacencies of cells * Cell adjacency seems the most complicated to generalise; however tuple indices may allow for * Numba is DIFFICULT to work with; it has the type stringency of a C language w/out significantly useful error messages\nFINALLY got the numba stuff working; still needs optimisatin; however everything is working in a generalised sense; I think a general N dimensional config object is doable now:\n\nstandardTest: 130.6 s\nThis is significantly worse than before; I thing its because of the generalised adjacency changes? Alternatively my laptop is on low battery with other stuff running on background; so may be that instead?\nTBD another day\n\nIf it’s a genuine slowdown, may need to comb through and see if its possible to optimise\nAdjacency code - may need to do that once per dimension case? May help reduce computation? Idk\n\n\n\n\n\nQuick test; still slow on full battery/restarted laptop; needs optimisation\n\nspliting adjacency function “getNeighbourIndices” into two:\n\nGet adjacency maps (array of addition/subtractions to be made) - can be done on basis of dimension once per config object\nAdjacency maps can then be passed into “getNeighbourIndices” to add maps%size to desired index\nSplitting computation should reduce repeat workload of finding adjacency maps\n\ngetArrayVal; can rewrite for loop in terms of array manipulations; unsure if it’d be faster given compilation, but numpy may have more efficient implementations\n\nMaybe setup some default D=1,2,3,4,5 search cases; may be faster for defaults that are likely going to be used in this lab; then remaining code\ngetArrayVal uses a size^{currentDimension} array; it’s the same for configs of same dimension/size; precomputing this for each config may help (instead of recomputing it for each\nflattenArray step may be movable to “getNeighbours” function instead? Demodularises the code, but would reduce repeat operations - could also pass in dimensionality as argument instead of evaluating it locally (again reducing repeat steps)\n\nMaybe combine several functions - I wonder if calls between numba’d functions reverts to python, requiring recompilation and slowing down results\n\nPlan:\n\nCombine config2D into config; code should work generalised:\n\nCareful w/2D indices in mcMove and calcEnergy; need to generalize this\n\nEnsure config object has information about its dimensionality\nHave config object query its own adjacency maps in the constructor (to avoid repetition)\nHave adjacency maps passed into Mcmove, and into getNeighbourIndices\nCombine functionality of getArrayVal into getNeighbours - leave getArrayVal as is for now (may be useful for some general one off computations) but streamline getNeighbours for efficiency as outlined above\nTest\nIf need be, maybe combine getNeighbourIndices into getNeighbours - if call between functions is slowing numba down, this is an efficiency gain (at the cost of modularity and legibility)\n\nIf need be combine this all into mcMove directly; preffer not to; modular code tends to be more legible, but it may be a worthwhile tradeoff; esp given that this code is the “core” of the simulation, and shouldn’t need to be touched once its working properlyi\n\n\n\n\n\nProblem - editing state Nd array is problematic w/numba\n\nWorkaround - don’t edit Nd array in mcMove; have mcMove return list of positions and associated values to run simulation; run simulation can then handle the editing\n\nHave found workaround; involves array.reshape(tuple) instead of np.reshape(array, array) - does require passing through sizes array as tuple; but that can be saved once by the config\n\n\nHave moved some functionality around\nstandardTest500; 83s - better but still not good\nEdited main loop in Mcmove to loop over cells in config rather than over I, j - generalises loop\nstandardTest500; 8.8s - seems that editing main loop vastly improved performance\nQuick test of 1D; broke - have noted #TODO in code\n\n\n\n\n\n\nSeems that editing main loop didn’t iterate over all elements?\n\nFixed it; idk if there was actually a speedup\nstandardTest500; 96s\n\nHave yet to separate adjacency check out\n\nHave seperated out “getNeighbourIndices” into “getNeighbourIndices” and “getAdjacencies” - “getAdjacencies” finds the relative neighbour indices (I.E, [0, +-1]) for a given dimensionality - this is done once per config object and passed through to “getNeighbourIndices” - should reduce computation\n\nSlight improvement: standardTest500: 92s\n\n\nWondering if “nested function calls” is whats causing issues?\n\nQuickly moving everything into mcMove - is a lot less modular, but may be more efficient, so worth testing standardTest500: 87.4s Slight improvement - I suspect not duplicating the array flattening step is improving things More tweaking: standardTest500: 81.2s\n\nMore tweaking:\nstandardTest500: 62.1s\nMore tweaking (merged editArray into mcMoves)\nstandardTest500: 33.3s\nMore tweaking - changes how e^(−cost β)=〖(e〗(−β))(cost) is computed - precomputing 〖(e〗^(−β)) to try and shave some more efficiency\nstandardTest500: 50s; have reverted change - presumably multiplication and one exp is easier on computer than alternative\nMinor testing; commented out “appendConfig(curConfig)” from “runSimulation” - given 500 steps, that gets called a lot; want to quantify how significant a drain it represents\nstandardTest500: 23.4s □ Does represent a not insigificant improvement □ May be including a “do not save” option to runSimulation - have added - should help w/both run times and file sizes\n\n\nTaken a break from optimisation to debug; have gotten it working in 1D case\nGeneral debugging complete; have quickly tested that 1D, 3D and 4D cases work; they do; have added graphing method to handle 3D case and rearranged graphing code a bit\nCode still needs commenting/general tidying, but it is in a baseline “complete” state\nNote; observing significant decrease in speed for higher dimension; setting up new test case:\nstandardTest3D100 = 20x20x20 over 100 steps\nstandardTest3D100: 562.6s (9.3min)\nThis is a significant increase from a 2D case; I suspect this will be significantly worse for even higher dimensions; as such it may be prudent to try and understand finite scaling effects in lower dimensions, and try and extrapolate that understanding to small higher dimensionality models as necessary\nNote; incorrect implenentation of code (removing np.flip(currentIndex) operation on line 215) creates an incredibly interesting behaviour which includes:\n\nSymetry along the x/y axis 3 distinct cluster types; black, white, crosshatched",
    "crumbs": [
      "Wk1",
      "Wk 1.1 Lab log"
    ]
  },
  {
    "objectID": "Wk1_22_01_2026/1_1_labLog.html#potential-plan-written-21012026",
    "href": "Wk1_22_01_2026/1_1_labLog.html#potential-plan-written-21012026",
    "title": "1.1 lab session notes",
    "section": "",
    "text": "Lab induction\nDr Turci walks us through introduction\nDiscuss refactoring code to give us a “complete virtual lab apparatus”:\nA complete system which we can use to generate arbitrary configuration datasets for study\nSetup github repo\nStart refactoring/writing code (hopefully finish this quickly - that way rest of lab can be running simulations w/minimal need for more coding)\n\nReasoning for this:\n\nWhat is needed from code seems fairly clear at this point; need to be able to generate arbitrary configurations simulated to equilibration; need to be able to extract arbitrary observables; need to be able to assemble these into datasets over parameter variation\nCurrently still going through “percolation” chapter - understanding of theory isn’t fully there yet, however understanding of code requirements ~ is - therefore start with what is known, and familiarise self with theory more later",
    "crumbs": [
      "Wk1",
      "Wk 1.1 Lab log"
    ]
  },
  {
    "objectID": "Wk1_22_01_2026/1_1_labLog.html#lab-log-22012026",
    "href": "Wk1_22_01_2026/1_1_labLog.html#lab-log-22012026",
    "title": "1.1 lab session notes",
    "section": "",
    "text": "Turci introduced us to project: * Gave us overview\nMet up w/lab partner, are trying to figure out project direction and logistical questions: * Setup shared google drive folder to share materials * Setup planning document * General starting aims: * Find critical exponents * Study cluster behavior * Vary over temperature and dimensionality * Agreed to refactor code in terms of config object * Object oriented config object w/pickle library save/load functionality - given that sims may be computationally intensive, saving results for future reanalysis may save compute time * Decided to use inheritance structure; parent config, and 1D, 2D, 3D, 4D config children - sim steps will look different in different dimensions and may be difficult to generalize - hence inheritance structure\nTalked to Turci: * Need to figure out when state converges\n\nSetup code refactoring thus far:\n\nParent config class - has saving/load methods\nSetup 2D config as test child case\n\n\nHave setup code in object oriented approach: Abstract config class 2D config class Have done rudimentary testing\nTo do list: * FIX STATE LIST\n\nClean up code\nImplement higher and lower dimensions classes\nRead up and figure out critical exponents\nOptimisation and factorisation:\n\nFigure out numba python library; speed up system\nMaybe rework observable lists into pandas dataframe? May be more generalisable/easier to work with\nDouble check observable calculations\nAdd “average observables” - need to be able to average over several iterations\nMaybe save multiple config steps?\n\nPotentially setup “dataset” object? Set of config objects along some varied quantity? Save to folder of\nSetup github quarto lab book; switch over to lab book\n\n##Lab log (24/01/2026):\n\nDecided to switch back to original state system for now; ik it works; may need to refactor it later\nGonna try and implement numba on 2D case; test everything in 2D, then can expand out to higher dimensions\nNvm; gonna try and get good bones in, and save all states\n\nFor current use, two general test cases:\n\nstandardTest2000 = Ising model 100x100 cells run for 2000 steps\nstandardTest500 = Ising model 100x100 cells run for 500 steps\nHave implemented saving all states - it does make the saved files significantly larger (I.E, 100x100 model w/100 steps, saving energy/magnetization observables is 32.5Mb) - quick test standardTest2000 s; took several minutes (didn’t time it, but took a while), saved to 192.5 Mb\n\nCurrently that is acceptable; (given 100x100x2000 = 2x10^7 stored cells)) that suggests each cell uses ~20 bytes of storage space; so extrapolating to a 3D case 100^3⋅2000=40Gb - a 4D case may be prohibitive w/~4Tb storage space for the same size;\n\nCould add discard functionality (I.E, don’t need the initial equilibration steps)\nCould only save the observables over that simulation length\nHowever for now having the default “save all raw data” option is good to have\n\n\nThinking of optimisations:\n\nmcMove method\ncalcEnergy method\nBoth are VERY inefficient; in similar ways\nBoth run through entire configuration; both check adjacent cells; currently implemented through for loops; however some steps may be more efficiently done by either numpy array operations, or numba implementations\nIt may also be interesting to see if adjacency checks could be generalised/factorised to any dimension; if that’s possible, inheritance structure becomes unecessary, as the algorithm could be implemented for all dimensionalities\n\nIt would be very useful if generalising is possible; this would allow detailed study of higher dimensional ising models\n\nConfig state storage - currently an array of integer values; am wondering if can switch to a binary representation - would make storage of saved files more efficient (I.E, 4D 100x100x2000 case could optimistically be reduced to 200Gb)\nData discard system - options to discard all but the N most recent configurations - storage data optimisation\n\n\nIf its possible to generalise the dinemsionality of the model; that opens up more lines of investigation\nUnsure which optimisations are worth it. Plan of action:\n\nTry and get numba working\nFamiliarise self w/numba operations\nTry and refactor/optimise above with numba in mind\n\nGot numba working; doing rudimentary tests to see if it improves performance:\n\nAdding numba decorator to mcMove method\nstandardTest500 yields Dt = 96.8 s\nstandardTest500 w/out numba yields: 92.2 s\nThis suggests no significant speedup\n\nSuggests I’m using it incorrectly\n\nTesting standardTest500 w/ provided code;\n\nw/out numba: 38.8 s\nW/ numba: 4.26 s\n\nMaybe object oriented approach is breaking stuff?\n\nChanged mc move; moved all self checks to runSimulation instead; that way mcMove is more “anonymous” and doesn’t need to call self\n\nstandardTest500: 47.8 s\nSignificant improvement\n\nWondering if difference between provided code and own code is due to “additional” code run by “simulationRun” - have commented out plotting and appending observables\n\nstandardTest500: 7.3 s\n\n\nTakeaways:\n\nSince Numba compiles individual functions to machine code; it is advisable for numba accelerated functions/methods to be “streamlined” w/out needing to refer to non accelerated code\n\nUnsure if numba accelerated functions can refer to other accelerated functions\n\nTest case: adjacency code in mcMove; move into separate accelerated method\n\nCan move it to separate function, but numba gets confused if it’s a class method\nstandardTest500: 6.9 s\n\n\n\nNumba seems to work; moving onto some architecture changes: * Current plan is to have inheritance structure w/specific dimension configs hard coded * 3 main differences between dimensions: Dimension of array First initial state Adjacencies of cells * Cell adjacency seems the most complicated to generalise; however tuple indices may allow for * Numba is DIFFICULT to work with; it has the type stringency of a C language w/out significantly useful error messages\nFINALLY got the numba stuff working; still needs optimisatin; however everything is working in a generalised sense; I think a general N dimensional config object is doable now:\n\nstandardTest: 130.6 s\nThis is significantly worse than before; I thing its because of the generalised adjacency changes? Alternatively my laptop is on low battery with other stuff running on background; so may be that instead?\nTBD another day\n\nIf it’s a genuine slowdown, may need to comb through and see if its possible to optimise\nAdjacency code - may need to do that once per dimension case? May help reduce computation? Idk",
    "crumbs": [
      "Wk1",
      "Wk 1.1 Lab log"
    ]
  },
  {
    "objectID": "Wk1_22_01_2026/1_1_labLog.html#section",
    "href": "Wk1_22_01_2026/1_1_labLog.html#section",
    "title": "1.1 lab session notes",
    "section": "",
    "text": "Quick test; still slow on full battery/restarted laptop; needs optimisation\n\nspliting adjacency function “getNeighbourIndices” into two:\n\nGet adjacency maps (array of addition/subtractions to be made) - can be done on basis of dimension once per config object\nAdjacency maps can then be passed into “getNeighbourIndices” to add maps%size to desired index\nSplitting computation should reduce repeat workload of finding adjacency maps\n\ngetArrayVal; can rewrite for loop in terms of array manipulations; unsure if it’d be faster given compilation, but numpy may have more efficient implementations\n\nMaybe setup some default D=1,2,3,4,5 search cases; may be faster for defaults that are likely going to be used in this lab; then remaining code\ngetArrayVal uses a size^{currentDimension} array; it’s the same for configs of same dimension/size; precomputing this for each config may help (instead of recomputing it for each\nflattenArray step may be movable to “getNeighbours” function instead? Demodularises the code, but would reduce repeat operations - could also pass in dimensionality as argument instead of evaluating it locally (again reducing repeat steps)\n\nMaybe combine several functions - I wonder if calls between numba’d functions reverts to python, requiring recompilation and slowing down results\n\nPlan:\n\nCombine config2D into config; code should work generalised:\n\nCareful w/2D indices in mcMove and calcEnergy; need to generalize this\n\nEnsure config object has information about its dimensionality\nHave config object query its own adjacency maps in the constructor (to avoid repetition)\nHave adjacency maps passed into Mcmove, and into getNeighbourIndices\nCombine functionality of getArrayVal into getNeighbours - leave getArrayVal as is for now (may be useful for some general one off computations) but streamline getNeighbours for efficiency as outlined above\nTest\nIf need be, maybe combine getNeighbourIndices into getNeighbours - if call between functions is slowing numba down, this is an efficiency gain (at the cost of modularity and legibility)\n\nIf need be combine this all into mcMove directly; preffer not to; modular code tends to be more legible, but it may be a worthwhile tradeoff; esp given that this code is the “core” of the simulation, and shouldn’t need to be touched once its working properlyi\n\n\n\n\n\nProblem - editing state Nd array is problematic w/numba\n\nWorkaround - don’t edit Nd array in mcMove; have mcMove return list of positions and associated values to run simulation; run simulation can then handle the editing\n\nHave found workaround; involves array.reshape(tuple) instead of np.reshape(array, array) - does require passing through sizes array as tuple; but that can be saved once by the config\n\n\nHave moved some functionality around\nstandardTest500; 83s - better but still not good\nEdited main loop in Mcmove to loop over cells in config rather than over I, j - generalises loop\nstandardTest500; 8.8s - seems that editing main loop vastly improved performance\nQuick test of 1D; broke - have noted #TODO in code",
    "crumbs": [
      "Wk1",
      "Wk 1.1 Lab log"
    ]
  },
  {
    "objectID": "Wk1_22_01_2026/1_1_labLog.html#log-1",
    "href": "Wk1_22_01_2026/1_1_labLog.html#log-1",
    "title": "1.1 lab session notes",
    "section": "",
    "text": "Seems that editing main loop didn’t iterate over all elements?\n\nFixed it; idk if there was actually a speedup\nstandardTest500; 96s\n\nHave yet to separate adjacency check out\n\nHave seperated out “getNeighbourIndices” into “getNeighbourIndices” and “getAdjacencies” - “getAdjacencies” finds the relative neighbour indices (I.E, [0, +-1]) for a given dimensionality - this is done once per config object and passed through to “getNeighbourIndices” - should reduce computation\n\nSlight improvement: standardTest500: 92s\n\n\nWondering if “nested function calls” is whats causing issues?\n\nQuickly moving everything into mcMove - is a lot less modular, but may be more efficient, so worth testing standardTest500: 87.4s Slight improvement - I suspect not duplicating the array flattening step is improving things More tweaking: standardTest500: 81.2s\n\nMore tweaking:\nstandardTest500: 62.1s\nMore tweaking (merged editArray into mcMoves)\nstandardTest500: 33.3s\nMore tweaking - changes how e^(−cost β)=〖(e〗(−β))(cost) is computed - precomputing 〖(e〗^(−β)) to try and shave some more efficiency\nstandardTest500: 50s; have reverted change - presumably multiplication and one exp is easier on computer than alternative\nMinor testing; commented out “appendConfig(curConfig)” from “runSimulation” - given 500 steps, that gets called a lot; want to quantify how significant a drain it represents\nstandardTest500: 23.4s □ Does represent a not insigificant improvement □ May be including a “do not save” option to runSimulation - have added - should help w/both run times and file sizes\n\n\nTaken a break from optimisation to debug; have gotten it working in 1D case\nGeneral debugging complete; have quickly tested that 1D, 3D and 4D cases work; they do; have added graphing method to handle 3D case and rearranged graphing code a bit\nCode still needs commenting/general tidying, but it is in a baseline “complete” state\nNote; observing significant decrease in speed for higher dimension; setting up new test case:\nstandardTest3D100 = 20x20x20 over 100 steps\nstandardTest3D100: 562.6s (9.3min)\nThis is a significant increase from a 2D case; I suspect this will be significantly worse for even higher dimensions; as such it may be prudent to try and understand finite scaling effects in lower dimensions, and try and extrapolate that understanding to small higher dimensionality models as necessary\nNote; incorrect implenentation of code (removing np.flip(currentIndex) operation on line 215) creates an incredibly interesting behaviour which includes:\n\nSymetry along the x/y axis 3 distinct cluster types; black, white, crosshatched",
    "crumbs": [
      "Wk1",
      "Wk 1.1 Lab log"
    ]
  },
  {
    "objectID": "Wk1_22_01_2026/1_2_WkSummary.html",
    "href": "Wk1_22_01_2026/1_2_WkSummary.html",
    "title": "Wk1.2 Summary",
    "section": "",
    "text": "Wk1.2 Summary",
    "crumbs": [
      "Wk1",
      "Wk 1.2 summary"
    ]
  },
  {
    "objectID": "testFolder/testpage.html",
    "href": "testFolder/testpage.html",
    "title": "testPage blah balh bal",
    "section": "",
    "text": "testPage blah balh bal\n\n\n\ntestImage\n\n\n%{python} %import os %print(\"hello world\") %print(os.getcwd()) %"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "t About this site"
  },
  {
    "objectID": "Wk2_29_01_2026/2_1_labLog.html",
    "href": "Wk2_29_01_2026/2_1_labLog.html",
    "title": "2.1 Lab session notes",
    "section": "",
    "text": "Have begun switching from onenote labbook to a quarto document. Still hand copy equations for “Wk0.5 background theory and concept”.\n\n\n\nTurci session; went over Jaynes model mean field theory derivion\ncatchup w/lab partner; outlined code She summarised lit review:\n\ncluster definition:\n\nuseful definition; connected points w/same spin; (domb and stall, link 2)\n\n\ncode plan:\n\ndataset object\n\nlist of config objects over temperature\nsave as folder of config objects - give config objects useful names to reconstruct dataset object\n\ngetClusters - returns list of clusters as some sort of data structure (maybe True/False bool array w/spin value?)\ncluster analysis methods:\n\ncluster size\ncorrelation length?\ncluster mean position? may not be useful for debugging?\ncluster spread (standard deviation) - measure of how spread out\n\n\ncluster selection algorithm outline:\nmay need two methods; one “find cluster from point” method and one “find all clusters” method\nfindAllClusters():\n\ncreate bool array of “checked” values (True represents cells that haven’t been checked yet, False represents cells that have been checked)\nwhile loop:\n\npick random “True” cell; assign to array indices\npropagate cluster from cell\nset “checked values” array position corresponding to new cluster to false\n\n\nfindCluster(cell, config); do as recursive algorithm\nchecked algorithm idea w/Scarlett; paper (https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.62.361)\nreorganised code into file structure - Main - cluster - testing\nthings that need doing: - cluster algorithm implementations - dataset object implementation - code cleanup\nScarlett will do cluster stuff, i’ll do dataset and code cleanup stuff.\ngot basic dataset constructor and saving/read to file functionality working\ndataset has a set of:\n\nequilibration steps - number of steps taken to equilibrate simulation - doesn’t save configs\nsimulation steps - number of steps during actual simulation\n\nobservables will be average of values over last simulation steps. implemented partition function, entropy, helmholtz free energy, as observables. implemented “average” observable methods - they get the average of a given observable between simulation indices Dataset can get standard observables from config now\n\n\n\n\nScarlett continues clustering analysis\nI continue dataset stuff and main code cleanup\n\nNEED to comment/document out code\n\ni read up on theory\n\n\n\n\nRead up on theory Scarlett reviewed during Wk1 and added relevant theory to (Wk 2.2 Additional reading). Have further read into some of the critical exponents theory as well in order to develop an understanding of how to compute them from our model. The critical exponent theory follows heavily from cite{CritExpLink3} which seems to handle scaling phenomena. As it stands, there may be two approaches to computing the critical exponents:\n\nSimple case; vary models over temperature and fit some variation of \\((T-T_{c})^{x}\\) where x is a critical parameter to various observables (as seen in cite{ComplexityAndCriticality} P132-134)\nAttempt to follow the procedure done by cite{CritExpLink3}\n\nThe former option is simpler and is a natural extension of current work w/dataset object; however am unsure if it’d accomodate finite scaling effects. It may be worth implementing the former case first in order to test the idea; furthermore graphing observables over temperature may give a better idea of the dynamics being explored; so even if this simpler method fails, it may yield experience and insight into the system’s behaviour.\nNote reading around topic briefly the “Swendsen-Wang” and “Wolff” algorithms are mentioned; they seem to be used to improve compute times near critical temperatures. Am unsure if they’d be necessary; but noting their existence/potential use case here in case they’d be useful later.\nA pure computational approach to extracting critical exponents may thus follow:\n\ncreate datasets of dimension d, size L over temperature ranges - try and implement a “critical temp finding” method which performs a binary search for the critical temperature in a data range for a given observable\nfit the simple critical exponent function to the data\nperform above for a range of sizes for same dimension; compare results\nif scaling seems predictable; computational approach may be sufficient, and exploration of higher dimension may begin\nelse will need to read up more on how to avoid scaling effects\n\nTherefore to do list follows:\n\ndataset object: finish off; it should be able to return a pandas dataframe of observables over temperature\nmay be useful to generalise dataset object so it may vary model size instead of temperature\nintroduce plotting methods for observables\nintroduce “critical temperature finding” methods - comparing derivatives of observables at extremal ends of data, should be able to determine if current location is above/below critical temperature - this would allow a binary search for the critical temperature to arbitrary precision\nfitting methods of observables to critical temp expressions\nfurther cleanup of code\n\n\n\n\ntidied up dataset object - introduced a to dataframe method; and cleaned up file saving system.\n\n\nStarting to generate experimental datasets to gain experience generating large datasets (naming format; :\n\ntestData20x20_1000_100: small 20x20 dataset run over 1000 eq steps, and simulated for 100 steps; range of 20 beta \\(\\in [0, 3]\\) - magnetisation doesn’t plot neatly\ntestData50x50_1000_100: small 50x50 dataset run over 1000 eq steps, and simulated for 100 steps; range of 20 beta \\(\\in [0, 3]\\) - magnetisation doesn’t plot neatly\n\nneither test case thus far plots neatly as seen:\n\n\n\nFig 2.1 - Magnetisation plotted over \\(\\beta\\) for testData50x50_1000_100\n\n\n\n\n\nFig 2.2 - Absolute Magnetisation plotted over \\(\\beta\\) for testData50x50_1000_100\n\n\nWhich shows a general increase in absolute magnetisation over \\(\\beta\\) (contrary to what intuition would suggest, with magnetisation increasing for low beta); with the occaisonal datapoint dropped to 0. Furthermore it is odd how absolute magnetisation increases, then suddenly drops as in fig 2.2\nThe final configuration states for this data was generated for visual inspection; some examples include:\n   \nOn initial inspection the low and high \\(\\beta\\) configs appear reasonable; fig.2.3 shows an orderly config for low \\(\\beta\\) whilst fig 2.6 shows a relatively disorded config. However the intermediate examples show a confusing switch in behaviour from relatively disorganised to organised (these correspond to one of the drops in fig 2.2).\nA few things become apparent comparing figs 2.3-2.6 to fig 2.1:\n\nmagnetisation computation is going wrong somehwhere; as highly ordered states (I.E fig 2.3) are reading low magnetisation whilst disorded stats (I.E fig 2.5) are reading high magnetisation. This contradicts an understanding of the underlying theory that ordered ising models should be more magnetised\nthe source of the discontinuous drops are unclear - there is a major drop at \\(\\beta\\approx 2.4\\) alongside several smaller drops; if those smaller drops didn’t exist, that large drop could be attributed to a phase transition\n\nThe random drops may be due to noise effects (the magnetistion is averaged over the relatively few 100 simulation steps); however the fact that the magnetisation grows over beta isn’t clear.\nPerforming larger test:\n\ntestData50x50_1000_1000: note over range \\(\\beta \\in [0, 4]\\)\n\nam hoping this will fix some of the data variability and give a clearer picture of phase transitions (even if the magnetisation over \\(\\beta\\) relation is flipped)\nam an absolute idiot; \\(\\beta \\propto \\frac{1}{T}\\); so lower \\(\\beta\\) cooresponds to higher \\(T\\); so the pattern seen is as expected - nothing is broken…\n\n\n\nMay need to modify dataset object to allow different configs to have different equilibration times; cite{CritExpLink3} suggests that equilibration time may vary over parameters\nFurthermore may need to implement system to estimate equilibration times:\n\ndual system introduced by cite{CritExpLink3} - create 2 configs; one starting all spin up, one all spin down; compare magnetisations of two results until they are within error of each other\nmay be worth implementing as purpose built system - i suspect that the config code can be streamlined purely for the purpose of computing these equilibration times\nmay also be worth creating a “equilibrationTime dataset” - able to generate a set of dualConfigs identical to dataset parameters; and spit out equilibration times - maybe make it suck that it can linearly interpolate between equilibrationTime datapoints; would allow for sparser data",
    "crumbs": [
      "Wk2",
      "Wk 2.1 Lab log"
    ]
  },
  {
    "objectID": "Wk2_29_01_2026/2_1_labLog.html#note",
    "href": "Wk2_29_01_2026/2_1_labLog.html#note",
    "title": "2.1 Lab session notes",
    "section": "",
    "text": "Have begun switching from onenote labbook to a quarto document. Still hand copy equations for “Wk0.5 background theory and concept”.",
    "crumbs": [
      "Wk2",
      "Wk 2.1 Lab log"
    ]
  },
  {
    "objectID": "Wk2_29_01_2026/2_1_labLog.html#lab-log",
    "href": "Wk2_29_01_2026/2_1_labLog.html#lab-log",
    "title": "2.1 Lab session notes",
    "section": "",
    "text": "Turci session; went over Jaynes model mean field theory derivion\ncatchup w/lab partner; outlined code She summarised lit review:\n\ncluster definition:\n\nuseful definition; connected points w/same spin; (domb and stall, link 2)\n\n\ncode plan:\n\ndataset object\n\nlist of config objects over temperature\nsave as folder of config objects - give config objects useful names to reconstruct dataset object\n\ngetClusters - returns list of clusters as some sort of data structure (maybe True/False bool array w/spin value?)\ncluster analysis methods:\n\ncluster size\ncorrelation length?\ncluster mean position? may not be useful for debugging?\ncluster spread (standard deviation) - measure of how spread out\n\n\ncluster selection algorithm outline:\nmay need two methods; one “find cluster from point” method and one “find all clusters” method\nfindAllClusters():\n\ncreate bool array of “checked” values (True represents cells that haven’t been checked yet, False represents cells that have been checked)\nwhile loop:\n\npick random “True” cell; assign to array indices\npropagate cluster from cell\nset “checked values” array position corresponding to new cluster to false\n\n\nfindCluster(cell, config); do as recursive algorithm\nchecked algorithm idea w/Scarlett; paper (https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.62.361)\nreorganised code into file structure - Main - cluster - testing\nthings that need doing: - cluster algorithm implementations - dataset object implementation - code cleanup\nScarlett will do cluster stuff, i’ll do dataset and code cleanup stuff.\ngot basic dataset constructor and saving/read to file functionality working\ndataset has a set of:\n\nequilibration steps - number of steps taken to equilibrate simulation - doesn’t save configs\nsimulation steps - number of steps during actual simulation\n\nobservables will be average of values over last simulation steps. implemented partition function, entropy, helmholtz free energy, as observables. implemented “average” observable methods - they get the average of a given observable between simulation indices Dataset can get standard observables from config now",
    "crumbs": [
      "Wk2",
      "Wk 2.1 Lab log"
    ]
  },
  {
    "objectID": "Wk2_29_01_2026/2_1_labLog.html#stuff-for-next-week",
    "href": "Wk2_29_01_2026/2_1_labLog.html#stuff-for-next-week",
    "title": "2.1 Lab session notes",
    "section": "",
    "text": "Scarlett continues clustering analysis\nI continue dataset stuff and main code cleanup\n\nNEED to comment/document out code\n\ni read up on theory",
    "crumbs": [
      "Wk2",
      "Wk 2.1 Lab log"
    ]
  },
  {
    "objectID": "Wk2_29_01_2026/2_1_labLog.html#notes",
    "href": "Wk2_29_01_2026/2_1_labLog.html#notes",
    "title": "2.1 Lab session notes",
    "section": "",
    "text": "Read up on theory Scarlett reviewed during Wk1 and added relevant theory to (Wk 2.2 Additional reading). Have further read into some of the critical exponents theory as well in order to develop an understanding of how to compute them from our model. The critical exponent theory follows heavily from cite{CritExpLink3} which seems to handle scaling phenomena. As it stands, there may be two approaches to computing the critical exponents:\n\nSimple case; vary models over temperature and fit some variation of \\((T-T_{c})^{x}\\) where x is a critical parameter to various observables (as seen in cite{ComplexityAndCriticality} P132-134)\nAttempt to follow the procedure done by cite{CritExpLink3}\n\nThe former option is simpler and is a natural extension of current work w/dataset object; however am unsure if it’d accomodate finite scaling effects. It may be worth implementing the former case first in order to test the idea; furthermore graphing observables over temperature may give a better idea of the dynamics being explored; so even if this simpler method fails, it may yield experience and insight into the system’s behaviour.\nNote reading around topic briefly the “Swendsen-Wang” and “Wolff” algorithms are mentioned; they seem to be used to improve compute times near critical temperatures. Am unsure if they’d be necessary; but noting their existence/potential use case here in case they’d be useful later.\nA pure computational approach to extracting critical exponents may thus follow:\n\ncreate datasets of dimension d, size L over temperature ranges - try and implement a “critical temp finding” method which performs a binary search for the critical temperature in a data range for a given observable\nfit the simple critical exponent function to the data\nperform above for a range of sizes for same dimension; compare results\nif scaling seems predictable; computational approach may be sufficient, and exploration of higher dimension may begin\nelse will need to read up more on how to avoid scaling effects\n\nTherefore to do list follows:\n\ndataset object: finish off; it should be able to return a pandas dataframe of observables over temperature\nmay be useful to generalise dataset object so it may vary model size instead of temperature\nintroduce plotting methods for observables\nintroduce “critical temperature finding” methods - comparing derivatives of observables at extremal ends of data, should be able to determine if current location is above/below critical temperature - this would allow a binary search for the critical temperature to arbitrary precision\nfitting methods of observables to critical temp expressions\nfurther cleanup of code",
    "crumbs": [
      "Wk2",
      "Wk 2.1 Lab log"
    ]
  },
  {
    "objectID": "Wk2_29_01_2026/2_1_labLog.html#section",
    "href": "Wk2_29_01_2026/2_1_labLog.html#section",
    "title": "2.1 Lab session notes",
    "section": "",
    "text": "tidied up dataset object - introduced a to dataframe method; and cleaned up file saving system.\n\n\nStarting to generate experimental datasets to gain experience generating large datasets (naming format; :\n\ntestData20x20_1000_100: small 20x20 dataset run over 1000 eq steps, and simulated for 100 steps; range of 20 beta \\(\\in [0, 3]\\) - magnetisation doesn’t plot neatly\ntestData50x50_1000_100: small 50x50 dataset run over 1000 eq steps, and simulated for 100 steps; range of 20 beta \\(\\in [0, 3]\\) - magnetisation doesn’t plot neatly\n\nneither test case thus far plots neatly as seen:\n\n\n\nFig 2.1 - Magnetisation plotted over \\(\\beta\\) for testData50x50_1000_100\n\n\n\n\n\nFig 2.2 - Absolute Magnetisation plotted over \\(\\beta\\) for testData50x50_1000_100\n\n\nWhich shows a general increase in absolute magnetisation over \\(\\beta\\) (contrary to what intuition would suggest, with magnetisation increasing for low beta); with the occaisonal datapoint dropped to 0. Furthermore it is odd how absolute magnetisation increases, then suddenly drops as in fig 2.2\nThe final configuration states for this data was generated for visual inspection; some examples include:\n   \nOn initial inspection the low and high \\(\\beta\\) configs appear reasonable; fig.2.3 shows an orderly config for low \\(\\beta\\) whilst fig 2.6 shows a relatively disorded config. However the intermediate examples show a confusing switch in behaviour from relatively disorganised to organised (these correspond to one of the drops in fig 2.2).\nA few things become apparent comparing figs 2.3-2.6 to fig 2.1:\n\nmagnetisation computation is going wrong somehwhere; as highly ordered states (I.E fig 2.3) are reading low magnetisation whilst disorded stats (I.E fig 2.5) are reading high magnetisation. This contradicts an understanding of the underlying theory that ordered ising models should be more magnetised\nthe source of the discontinuous drops are unclear - there is a major drop at \\(\\beta\\approx 2.4\\) alongside several smaller drops; if those smaller drops didn’t exist, that large drop could be attributed to a phase transition\n\nThe random drops may be due to noise effects (the magnetistion is averaged over the relatively few 100 simulation steps); however the fact that the magnetisation grows over beta isn’t clear.\nPerforming larger test:\n\ntestData50x50_1000_1000: note over range \\(\\beta \\in [0, 4]\\)\n\nam hoping this will fix some of the data variability and give a clearer picture of phase transitions (even if the magnetisation over \\(\\beta\\) relation is flipped)\nam an absolute idiot; \\(\\beta \\propto \\frac{1}{T}\\); so lower \\(\\beta\\) cooresponds to higher \\(T\\); so the pattern seen is as expected - nothing is broken…\n\n\n\nMay need to modify dataset object to allow different configs to have different equilibration times; cite{CritExpLink3} suggests that equilibration time may vary over parameters\nFurthermore may need to implement system to estimate equilibration times:\n\ndual system introduced by cite{CritExpLink3} - create 2 configs; one starting all spin up, one all spin down; compare magnetisations of two results until they are within error of each other\nmay be worth implementing as purpose built system - i suspect that the config code can be streamlined purely for the purpose of computing these equilibration times\nmay also be worth creating a “equilibrationTime dataset” - able to generate a set of dualConfigs identical to dataset parameters; and spit out equilibration times - maybe make it suck that it can linearly interpolate between equilibrationTime datapoints; would allow for sparser data",
    "crumbs": [
      "Wk2",
      "Wk 2.1 Lab log"
    ]
  },
  {
    "objectID": "Wk0/0_3_Module_and_assignment_specifics.html",
    "href": "Wk0/0_3_Module_and_assignment_specifics.html",
    "title": "0.3 - Module and assignment specifics",
    "section": "",
    "text": "0.3 - Module and assignment specifics\nCourse details: Unit director: Dr Andrei Sarua (a.sarua@bristol.ac.uk) Technical support: Tom Kennedy (Tom.Kennedy@bristol.ac.uk)\nLab opening hours: 10:00-13:00 14:00-17:00 First lab date: 22/01/2026 Last lab date: 26/02/2026\nAssessment details: Lab book submission due date: 12:30 16/03/2026 Lab report submission due date: 12:30 16/03/2026 Assessed on: - In lab attendance/performance - lab book - final report Marking criteria: - Knowledge & understanding - Intellectual skills - Research & scientific practices - Professional & life skills\nLab details: Mab group: C (Thursday labs) Lab partner: Scarlett Kitchener (up24667@bristol.ac.uk) Lab experiment: Ising model Supervisor: Dr Francesco Turci (f.turci@bristol.ac.uk)\nGuidance on lab book: Digital lab book using onenote; should be continuous diary over project; make sure to copy any handwritten notes into the lab book. Needs to be submitted as exported PDF.\nGuidance on final report: Final report is scientific letter style paper using provided LaTex template. 4 pages total (inc everything, inc references/appendix). Best “example” of style to write in will be published scientific articles in area of study.\nLab handbook experiment description (): The Ising model represents a simplified magnet in which each atom has a spin, which can only take one of two values, ‘up’ and ‘down’. Historically, it played a fundamental role in the theory of phase transitions in statistical physics, since it goes from a non-magnetic state at high temperature to a magnetic state at low temperatures, below the Curie temperature Tc. But there are many surprises, and the model and its close relatives are still central to modern physics. For example, the 2016 Nobel prize was awarded to Kosterlitz and Thouless for the theory of phase transitions in the related XY model (where the spin is still fixed in size, but can have any direction in the x-y plane). The Ising model is discrete and so is particularly suited for investigation by computers. In this laboratory we shall investigate the thermodynamics of the Ising model using the Metropolis Monte- Carlo method. Using this approach, we shall investigate the Ising model in zero, one and two dimensions, seeing how the statistical state changes as temperature and magnetic field are varied. In particular, we will use the Metropolis method to investigate the ‘critical exponents’ in the two-dimensional model, which represent how the magnetization, energy, entropy, heat capacity, etc., change near to the Curie temperature Tc. Optionally, the role of the lattice geometry, dimensionality, and similar models, such as Potts and XY could also be explored. This laboratory is specific to theoretical physics students, and introduces concepts, which will be fundamental in the later, e.g., 4th year Phase Transitions course.",
    "crumbs": [
      "Wk0",
      "Wk0.3 Module and assignment specifics"
    ]
  },
  {
    "objectID": "Wk0/0_2_Bibliography.html",
    "href": "Wk0/0_2_Bibliography.html",
    "title": "0.2 Bibliography",
    "section": "",
    "text": "0.2 Bibliography\nA page to keep track of which references are used. Note table 0.1, 0.2 aren’t formatted citations, instead representing an easy way to keep track of references and the information needed to create formatted citations. Note that I intend to use a a python script in my personal library to convert tabulated reference information to .bib files.\nThe tables generally have the following fields: * CitationKey (internal reference name) * Entry type (article, book, online, misc) * Note (personal notes on the citation) * URL (preferably direct link to where I got it from) * ISBN (if relevant) * DOI (if relevant) * Author * Title * bookTitle * Publisher * Year * Journal\nCurrently all bilbiography information is stored in ‘mainBibliography.csv’ (unsure how to get quarto to display it here)",
    "crumbs": [
      "Wk0",
      "Wk0.2 Bibliography"
    ]
  },
  {
    "objectID": "Wk0/0_4_General_pre_prep_log.html",
    "href": "Wk0/0_4_General_pre_prep_log.html",
    "title": "0.4 General pre-prep log",
    "section": "",
    "text": "0.4 General pre-prep log\n15/01/2026\nAdded recommended textbook to bibliography .\nThis lab explores the dynamics of the ising model; a simple model magnet\n19/01/2026 Briefly discussed lab w/Dr Turci; he noted the “percolation” chapter in as a good starting point since it introduces many ideas relating to phase transitions that will be useful for this project\nBlackboard resources “ising1.pdf” have provided a general overview of background concepts and information. Useful ideas and concepts summarised in “0.5 background theory and concepts.” These are NOT the full background theory for this lab, but represent a useful starting point.\n21/01/2026 Called lab partner to introduce self; went over basic logistics and plans, are now on same page.\nBb materials “earmark” list:\n\n“ising1.pdf”, “ising2.pdf” - recaps basic stat mech theory, and covers basic setup of ising model\n“ising exercise.ppt” - notes metropolis algorithm, alongside some observables to compute\n“finite-size-effects.pdf” - goes over phase transition details - order parameters and critical exponents -\n\nLooking through more of the BB materials; and trying to think of vague plan:\n\nGo through BB “finite size effects.pdf”\nGo through “percolation” chapter - (in retrospect this may take a while) - note section 1.8 supposedly outlines extraction of critical exponents\nFamiliarize self w/python simulation provided\nRefactor code in a .py file - jupyter notebooks are more annoying to work with in my experience\n\nAdditional benefit: lets me use my personal library which may be useful\n\nRefactor/expand code “architecture” generally - some potential ideas:\n\nMAYBE rework simulation to allow arbitrary dimensionality (I.E, 1D, 2D, 3D, 4D…) - suspect there may be interesting comparisons for different dimensionalities\nRework as object orriented - each config could be it’s own object with observable methods (I.E, computeEnergy, magnetisation…) and simulation step method - a “simulation run” class could then act as a wrapper for many config objects over time\n\nMay also need to handle different simulations “in parallel” - I.E, need to vary sim parameter to generate data points\nMaybe set it up s.t configuration is object w/update method; overwrites itself; is a data destructive idea, however should only need a final “equilibrium” configuration for a given set of sim parameters\n\nThen a “dataset” object would be a list of final configs + some metadata (I.E, lattice size, # sim steps, dimensionality…)\n\n\nExport/import method - given the computational nature of this lab, heavy compute runs may occur - being able to save raw simulation runs data in a standard format may save compute time\nAdditional observable computation - current code computes energy and magnetisation - adding additional observables may be useful for investigation, I.E:\n\nHeat capacity\nSusceptibility\nHelmholtz free energy\n\n\nMaybe setup shared github repo w/lab partner",
    "crumbs": [
      "Wk0",
      "Wk0.4 General pre-prep log"
    ]
  }
]